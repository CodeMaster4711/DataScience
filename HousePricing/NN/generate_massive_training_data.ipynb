{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massive Data Augmentation Pipeline\n",
    "\n",
    "## Ziel:\n",
    "- Original: ~16,500 Training Samples\n",
    "- **Output: ~300,000+ Samples**\n",
    "\n",
    "## Augmentation Strategien:\n",
    "1. **Multi-Level Gaussian Noise** (10x)\n",
    "2. **SMOTE/ADASYN** (Synthetic Minority Oversampling)\n",
    "3. **Mixup Augmentation**\n",
    "4. **Feature-wise Perturbation**\n",
    "\n",
    "## Warum mehr Daten?\n",
    "- Bessere Generalisierung\n",
    "- Robusteres Lernen\n",
    "- Verhindert Overfitting\n",
    "- Deep Learning braucht viele Daten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries loaded\n",
      "\n",
      "ðŸ“Š Data Augmentation Pipeline Starting...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ Libraries loaded\")\n",
    "print(\"\\nðŸ“Š Data Augmentation Pipeline Starting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: (20640, 10)\n",
      "âœ“ Geographic features added\n"
     ]
    }
   ],
   "source": [
    "# ===== LOAD DATA + v3 FEATURE ENGINEERING =====\n",
    "housing = pd.read_csv(\"../housing.csv\")\n",
    "print(f\"Original Data: {housing.shape}\")\n",
    "\n",
    "# Geografisches Clustering\n",
    "kmeans = KMeans(n_clusters=15, random_state=42, n_init=10)\n",
    "housing['geo_cluster'] = kmeans.fit_predict(housing[['latitude', 'longitude']])\n",
    "\n",
    "# KNN Nachbarschafts-Features\n",
    "knn = NearestNeighbors(n_neighbors=11)\n",
    "knn.fit(housing[['latitude', 'longitude']])\n",
    "distances, indices = knn.kneighbors(housing[['latitude', 'longitude']])\n",
    "\n",
    "neighbor_prices = []\n",
    "neighbor_income = []\n",
    "for idx_list in indices:\n",
    "    neighbor_idx = idx_list[1:]\n",
    "    neighbor_prices.append(housing.iloc[neighbor_idx]['median_house_value'].mean())\n",
    "    neighbor_income.append(housing.iloc[neighbor_idx]['median_income'].mean())\n",
    "\n",
    "housing['avg_neighbor_price'] = neighbor_prices\n",
    "housing['avg_neighbor_income'] = neighbor_income\n",
    "housing['avg_neighbor_distance'] = distances[:, 1:].mean(axis=1)\n",
    "\n",
    "print(\"âœ“ Geographic features added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Feature Engineering Complete: 42 features\n"
     ]
    }
   ],
   "source": [
    "# ===== COMPLETE v3 FEATURE ENGINEERING =====\n",
    "def create_v3_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basis Features\n",
    "    df['rooms_per_household'] = df['total_rooms'] / df['households']\n",
    "    df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']\n",
    "    df['population_per_household'] = df['population'] / df['households']\n",
    "    df['rooms_per_person'] = df['total_rooms'] / (df['population'] + 1)\n",
    "    df['bedrooms_per_household'] = df['total_bedrooms'] / df['households']\n",
    "    \n",
    "    # Polynomial\n",
    "    df['median_income_squared'] = df['median_income'] ** 2\n",
    "    df['median_income_cubed'] = df['median_income'] ** 3\n",
    "    df['age_squared'] = df['housing_median_age'] ** 2\n",
    "    \n",
    "    # Interactions\n",
    "    df['income_per_room'] = df['median_income'] / (df['total_rooms'] + 1)\n",
    "    df['income_per_person'] = df['median_income'] / (df['population'] + 1)\n",
    "    df['income_times_age'] = df['median_income'] * df['housing_median_age']\n",
    "    df['lat_long'] = df['latitude'] * df['longitude']\n",
    "    \n",
    "    # Log Transforms\n",
    "    df['log_total_rooms'] = np.log1p(df['total_rooms'])\n",
    "    df['log_population'] = np.log1p(df['population'])\n",
    "    df['log_median_income'] = np.log1p(df['median_income'])\n",
    "    \n",
    "    # City Distances\n",
    "    cities = {\n",
    "        'sf': (37.77, -122.41),\n",
    "        'la': (34.05, -118.24),\n",
    "        'san_diego': (32.72, -117.16),\n",
    "        'sacramento': (38.58, -121.49)\n",
    "    }\n",
    "    \n",
    "    for city_name, (lat, lon) in cities.items():\n",
    "        df[f'distance_to_{city_name}'] = np.sqrt(\n",
    "            (df['latitude'] - lat)**2 + (df['longitude'] - lon)**2\n",
    "        )\n",
    "    \n",
    "    distance_cols = [f'distance_to_{city}' for city in cities.keys()]\n",
    "    df['min_distance_to_city'] = df[distance_cols].min(axis=1)\n",
    "    \n",
    "    # Economic\n",
    "    df['is_coastal'] = df['ocean_proximity'].isin(['NEAR BAY', 'NEAR OCEAN', '<1H OCEAN']).astype(int)\n",
    "    df['wealth_index'] = df['median_income'] * df['rooms_per_household'] * (1 + df['is_coastal'] * 0.3)\n",
    "    df['population_density'] = df['population'] / (df['total_rooms'] + 1)\n",
    "    df['quality_score'] = (\n",
    "        df['rooms_per_household'] * 0.3 +\n",
    "        df['median_income'] * 0.5 +\n",
    "        df['is_coastal'] * 0.2\n",
    "    )\n",
    "    \n",
    "    # Age Features\n",
    "    df['is_new'] = (df['housing_median_age'] <= 10).astype(int)\n",
    "    df['is_old'] = (df['housing_median_age'] >= 40).astype(int)\n",
    "    \n",
    "    # Binning\n",
    "    df['lat_bin'] = pd.cut(df['latitude'], bins=10, labels=False)\n",
    "    df['long_bin'] = pd.cut(df['longitude'], bins=10, labels=False)\n",
    "    \n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "housing = create_v3_features(housing)\n",
    "print(f\"âœ“ Feature Engineering Complete: {housing.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Original Split:\n",
      "  Train: 16,512 samples\n",
      "  Test:  4,128 samples\n",
      "\n",
      "ðŸš€ Starting Augmentation Pipeline...\n"
     ]
    }
   ],
   "source": [
    "# ===== PREPARE FOR AUGMENTATION =====\n",
    "X = housing.drop('median_house_value', axis=1)\n",
    "y = housing['median_house_value']\n",
    "\n",
    "# Train/Test Split (nur Train augmentieren!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Original Split:\")\n",
    "print(f\"  Train: {len(X_train):,} samples\")\n",
    "print(f\"  Test:  {len(X_test):,} samples\")\n",
    "print(f\"\\nðŸš€ Starting Augmentation Pipeline...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation 1: Multi-Level Gaussian Noise\n",
    "\n",
    "Verschiedene Noise Levels = verschiedene \"Perspektiven\" auf die Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] Applying Multi-Level Gaussian Noise...\n",
      "  Original: 16,512 â†’ Gaussian: 181,632 (+165,120)\n",
      "  Augmentation Factor: 11.0x\n"
     ]
    }
   ],
   "source": [
    "# ===== GAUSSIAN NOISE AUGMENTATION =====\n",
    "def gaussian_augmentation_multilevel(X, y):\n",
    "    \"\"\"\n",
    "    Multi-Level Gaussian Noise Augmentation\n",
    "    \n",
    "    Verschiedene Noise Levels:\n",
    "    - Low (1%): 3x Kopien - sehr Ã¤hnlich zu Original\n",
    "    - Medium (3%): 4x Kopien - moderate Variation  \n",
    "    - High (5%): 2x Kopien - grÃ¶ÃŸere Variation\n",
    "    - Very High (7%): 1x Kopien - maximale Variation\n",
    "    \"\"\"\n",
    "    # One-Hot Encoding\n",
    "    X_encoded = pd.get_dummies(X, columns=['ocean_proximity'], drop_first=False)\n",
    "    \n",
    "    # Imputation\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X_encoded),\n",
    "        columns=X_encoded.columns\n",
    "    )\n",
    "    \n",
    "    X_list = [X_imputed.values]\n",
    "    y_list = [y.values]\n",
    "    \n",
    "    # Feature std for realistic noise\n",
    "    feature_std = np.std(X_imputed.values, axis=0)\n",
    "    \n",
    "    configs = [\n",
    "        {'noise_level': 0.01, 'copies': 3, 'y_var': 0.005},  # 1% noise\n",
    "        {'noise_level': 0.03, 'copies': 4, 'y_var': 0.01},   # 3% noise\n",
    "        {'noise_level': 0.05, 'copies': 2, 'y_var': 0.015},  # 5% noise\n",
    "        {'noise_level': 0.07, 'copies': 1, 'y_var': 0.02},   # 7% noise\n",
    "    ]\n",
    "    \n",
    "    for config in configs:\n",
    "        for _ in range(config['copies']):\n",
    "            # X noise\n",
    "            X_noise = np.random.normal(0, config['noise_level'], X_imputed.shape) * feature_std\n",
    "            X_noisy = X_imputed.values + X_noise\n",
    "            \n",
    "            # Y noise\n",
    "            y_noise = np.random.normal(1.0, config['y_var'], y.shape)\n",
    "            y_noisy = y.values * y_noise\n",
    "            \n",
    "            X_list.append(X_noisy)\n",
    "            y_list.append(y_noisy)\n",
    "    \n",
    "    X_augmented = np.vstack(X_list)\n",
    "    y_augmented = np.hstack(y_list)\n",
    "    \n",
    "    return X_augmented, y_augmented, X_encoded.columns\n",
    "\n",
    "print(\"\\n[1/4] Applying Multi-Level Gaussian Noise...\")\n",
    "X_gauss, y_gauss, feature_names = gaussian_augmentation_multilevel(X_train, y_train)\n",
    "print(f\"  Original: {len(X_train):,} â†’ Gaussian: {len(X_gauss):,} (+{len(X_gauss)-len(X_train):,})\")\n",
    "print(f\"  Augmentation Factor: {len(X_gauss)/len(X_train):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation 2: SMOTE (Synthetic Minority Over-sampling)\n",
    "\n",
    "Generiert synthetische Samples durch Interpolation zwischen Ã¤hnlichen Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/4] Applying SMOTE...\n",
      "  Before SMOTE: 181,632 â†’ After: 181,640 (+8)\n"
     ]
    }
   ],
   "source": [
    "# ===== SMOTE AUGMENTATION =====\n",
    "print(\"\\n[2/4] Applying SMOTE...\")\n",
    "\n",
    "# FÃ¼r Regression: Binne Target fÃ¼r SMOTE\n",
    "y_binned = pd.qcut(y_gauss, q=10, labels=False, duplicates='drop')\n",
    "\n",
    "try:\n",
    "    smote = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)\n",
    "    X_smote, y_binned_smote = smote.fit_resample(X_gauss, y_binned)\n",
    "    \n",
    "    # Rekonstruiere kontinuierliche y Werte\n",
    "    # FÃ¼r jedes neue Sample: durchschnitt von k nÃ¤chsten Nachbarn\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    knn_reg = NearestNeighbors(n_neighbors=5)\n",
    "    knn_reg.fit(X_gauss)\n",
    "    \n",
    "    # Nur neue Samples (nach len(X_gauss))\n",
    "    n_new = len(X_smote) - len(X_gauss)\n",
    "    X_new_smote = X_smote[len(X_gauss):]\n",
    "    \n",
    "    y_new_smote = []\n",
    "    for x_new in X_new_smote:\n",
    "        distances, indices = knn_reg.kneighbors([x_new])\n",
    "        y_new_smote.append(y_gauss[indices[0]].mean())\n",
    "    \n",
    "    # Kombiniere\n",
    "    X_smote_final = np.vstack([X_gauss, X_new_smote])\n",
    "    y_smote_final = np.hstack([y_gauss, np.array(y_new_smote)])\n",
    "    \n",
    "    print(f\"  Before SMOTE: {len(X_gauss):,} â†’ After: {len(X_smote_final):,} (+{n_new:,})\")\n",
    "    \n",
    "    X_current = X_smote_final\n",
    "    y_current = y_smote_final\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  SMOTE failed: {e}\")\n",
    "    print(f\"  Continuing without SMOTE...\")\n",
    "    X_current = X_gauss\n",
    "    y_current = y_gauss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation 3: Mixup\n",
    "\n",
    "Mischt zwei Samples um glattere Decision Boundaries zu lernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/4] Applying Mixup...\n",
      "  Before Mixup: 181,640 â†’ After: 211,640 (+30,000)\n"
     ]
    }
   ],
   "source": [
    "# ===== MIXUP AUGMENTATION =====\n",
    "print(\"\\n[3/4] Applying Mixup...\")\n",
    "\n",
    "def mixup_augmentation(X, y, n_mixup=20000, alpha_range=(0.2, 0.8)):\n",
    "    \"\"\"\n",
    "    Mixup: X_new = alpha * X_i + (1-alpha) * X_j\n",
    "    \"\"\"\n",
    "    X_mixup_list = []\n",
    "    y_mixup_list = []\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    \n",
    "    for _ in range(n_mixup):\n",
    "        # WÃ¤hle zwei zufÃ¤llige Samples\n",
    "        i = np.random.randint(0, n_samples)\n",
    "        j = np.random.randint(0, n_samples)\n",
    "        \n",
    "        # Random mixing ratio\n",
    "        alpha = np.random.uniform(alpha_range[0], alpha_range[1])\n",
    "        \n",
    "        # Mix\n",
    "        X_mixed = alpha * X[i] + (1 - alpha) * X[j]\n",
    "        y_mixed = alpha * y[i] + (1 - alpha) * y[j]\n",
    "        \n",
    "        X_mixup_list.append(X_mixed)\n",
    "        y_mixup_list.append(y_mixed)\n",
    "    \n",
    "    X_mixup = np.array(X_mixup_list)\n",
    "    y_mixup = np.array(y_mixup_list)\n",
    "    \n",
    "    # Kombiniere\n",
    "    X_combined = np.vstack([X, X_mixup])\n",
    "    y_combined = np.hstack([y, y_mixup])\n",
    "    \n",
    "    return X_combined, y_combined\n",
    "\n",
    "X_mixup, y_mixup = mixup_augmentation(X_current, y_current, n_mixup=30000)\n",
    "print(f\"  Before Mixup: {len(X_current):,} â†’ After: {len(X_mixup):,} (+{len(X_mixup)-len(X_current):,})\")\n",
    "\n",
    "X_current = X_mixup\n",
    "y_current = y_mixup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation 4: Feature-wise Perturbation\n",
    "\n",
    "StÃ¶re nur spezifische wichtige Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/4] Applying Feature-wise Perturbation...\n",
      "  Before: 211,640 â†’ After: 226,640 (+15,000)\n"
     ]
    }
   ],
   "source": [
    "# ===== FEATURE-WISE PERTURBATION =====\n",
    "print(\"\\n[4/4] Applying Feature-wise Perturbation...\")\n",
    "\n",
    "def feature_perturbation(X, y, n_perturb=10000, feature_names=None):\n",
    "    \"\"\"\n",
    "    StÃ¶re nur wichtige Features einzeln\n",
    "    \"\"\"\n",
    "    X_perturb_list = []\n",
    "    y_perturb_list = []\n",
    "    \n",
    "    # Wichtige Features (basiert auf v3 Feature Importance)\n",
    "    important_features = [\n",
    "        'median_income', 'latitude', 'longitude', 'housing_median_age',\n",
    "        'total_rooms', 'population', 'households'\n",
    "    ]\n",
    "    \n",
    "    # Finde Indizes (ungefÃ¤hr, da One-Hot encoded)\n",
    "    for _ in range(n_perturb):\n",
    "        idx = np.random.randint(0, len(X))\n",
    "        x_sample = X[idx].copy()\n",
    "        \n",
    "        # WÃ¤hle zufÃ¤lliges Feature zum Perturbieren\n",
    "        feature_idx = np.random.randint(0, min(10, X.shape[1]))  # Nur numerische Features\n",
    "        \n",
    "        # Perturbiere mit 5-10%\n",
    "        perturbation = np.random.normal(1.0, 0.07)\n",
    "        x_sample[feature_idx] *= perturbation\n",
    "        \n",
    "        # Y auch leicht variieren\n",
    "        y_perturbed = y[idx] * np.random.normal(1.0, 0.02)\n",
    "        \n",
    "        X_perturb_list.append(x_sample)\n",
    "        y_perturb_list.append(y_perturbed)\n",
    "    \n",
    "    X_combined = np.vstack([X, np.array(X_perturb_list)])\n",
    "    y_combined = np.hstack([y, np.array(y_perturb_list)])\n",
    "    \n",
    "    return X_combined, y_combined\n",
    "\n",
    "X_final, y_final = feature_perturbation(X_current, y_current, n_perturb=15000)\n",
    "print(f\"  Before: {len(X_current):,} â†’ After: {len(X_final):,} (+{len(X_final)-len(X_current):,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA AUGMENTATION COMPLETE\n",
      "============================================================\n",
      "Original Train Samples:      16,512\n",
      "After Augmentation:         226,640\n",
      "Augmentation Factor:           13.7x\n",
      "Features:                        45\n",
      "============================================================\n",
      "\n",
      "âœ“ Saved:\n",
      "  - X_train_augmented_300k.npy\n",
      "  - y_train_augmented_300k.npy\n",
      "  - feature_names.npy\n",
      "\n",
      "ðŸŽ¯ Ready for Neural Network Training!\n"
     ]
    }
   ],
   "source": [
    "# ===== FINAL SUMMARY =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA AUGMENTATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original Train Samples:  {len(X_train):>10,}\")\n",
    "print(f\"After Augmentation:      {len(X_final):>10,}\")\n",
    "print(f\"Augmentation Factor:     {len(X_final)/len(X_train):>10.1f}x\")\n",
    "print(f\"Features:                {X_final.shape[1]:>10}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save to file\n",
    "np.save('X_train_augmented_300k.npy', X_final)\n",
    "np.save('y_train_augmented_300k.npy', y_final)\n",
    "np.save('feature_names.npy', feature_names.values)\n",
    "\n",
    "print(\"\\nâœ“ Saved:\")\n",
    "print(\"  - X_train_augmented_300k.npy\")\n",
    "print(\"  - y_train_augmented_300k.npy\")\n",
    "print(\"  - feature_names.npy\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Ready for Neural Network Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Test Set saved (no augmentation):\n",
      "  Test Samples: 4,128\n"
     ]
    }
   ],
   "source": [
    "# ===== PREPARE TEST SET (NO AUGMENTATION!) =====\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=['ocean_proximity'], drop_first=False)\n",
    "\n",
    "# Align columns\n",
    "for col in feature_names:\n",
    "    if col not in X_test_encoded.columns:\n",
    "        X_test_encoded[col] = 0\n",
    "X_test_encoded = X_test_encoded[feature_names]\n",
    "\n",
    "# Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer_test = SimpleImputer(strategy='median')\n",
    "X_test_final = imputer_test.fit_transform(X_test_encoded)\n",
    "\n",
    "# Save Test Set\n",
    "np.save('X_test.npy', X_test_final)\n",
    "np.save('y_test.npy', y_test.values)\n",
    "\n",
    "print(\"âœ“ Test Set saved (no augmentation):\")\n",
    "print(f\"  Test Samples: {len(X_test_final):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
