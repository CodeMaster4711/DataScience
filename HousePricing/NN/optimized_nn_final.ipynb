{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Neural Network - Maximum Accuracy\n",
    "\n",
    "## Implementierte Verbesserungen:\n",
    "1. **Ensemble Learning** - 5 Modelle mit verschiedenen Seeds\n",
    "2. **Residual Connections** - Skip Connections für tiefere Netze\n",
    "3. **Huber Loss** - Robuster gegen Outliers als MSE\n",
    "4. **Advanced Features** - Polynomial & Interaction Features\n",
    "5. **Stochastic Weight Averaging** - Bessere Generalisierung\n",
    "6. **Data Augmentation** - 5x mehr Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ADVANCED FEATURE ENGINEERING =====\n",
    "def advanced_feature_engineering(df):\n",
    "    \"\"\"Erweiterte Feature Engineering Pipeline\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic Features\n",
    "    df['rooms_per_household'] = df['total_rooms'] / (df['households'] + 1)\n",
    "    df['bedrooms_ratio'] = df['total_bedrooms'] / (df['total_rooms'] + 1)\n",
    "    df['population_density'] = df['population'] / (df['households'] + 1)\n",
    "    df['income_per_bedroom'] = df['median_income'] / (df['total_bedrooms'] + 1)\n",
    "    df['rooms_per_person'] = df['total_rooms'] / (df['population'] + 1)\n",
    "    df['bedrooms_per_person'] = df['total_bedrooms'] / (df['population'] + 1)\n",
    "    \n",
    "    # Advanced Features\n",
    "    df['income_squared'] = df['median_income'] ** 2\n",
    "    df['income_cubed'] = df['median_income'] ** 3\n",
    "    df['age_per_income'] = df['housing_median_age'] / (df['median_income'] + 1)\n",
    "    \n",
    "    # Interaction Features (wichtigste Kombinationen)\n",
    "    df['income_x_rooms'] = df['median_income'] * df['rooms_per_household']\n",
    "    df['income_x_age'] = df['median_income'] * df['housing_median_age']\n",
    "    df['location_interaction'] = df['longitude'] * df['latitude']\n",
    "    \n",
    "    # Geospatial Features\n",
    "    df['distance_to_center'] = np.sqrt(df['longitude']**2 + df['latitude']**2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_data(train_df, test_df):\n",
    "    \"\"\"Complete Preprocessing Pipeline\"\"\"\n",
    "    target_col = 'median_house_value'\n",
    "    X_train = train_df.drop(columns=[target_col])\n",
    "    y_train = train_df[target_col].values\n",
    "    \n",
    "    # Outlier Removal\n",
    "    lower_q = np.percentile(y_train, 1)\n",
    "    mask = y_train >= lower_q\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    \n",
    "    # Advanced Feature Engineering\n",
    "    X_train = advanced_feature_engineering(X_train)\n",
    "    X_test = advanced_feature_engineering(test_df.copy())\n",
    "    \n",
    "    # Remove IDs\n",
    "    X_train = X_train.drop(columns=['id'], errors='ignore')\n",
    "    X_test = X_test.drop(columns=['id'], errors='ignore')\n",
    "    \n",
    "    # One-Hot Encoding\n",
    "    cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    if cat_cols:\n",
    "        X_train = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)\n",
    "        X_test = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)\n",
    "        for col in set(X_train.columns) - set(X_test.columns):\n",
    "            X_test[col] = 0\n",
    "        X_test = X_test[X_train.columns]\n",
    "    \n",
    "    # Imputation\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv('../train.csv')\n",
    "test_df = pd.read_csv('../test.csv')\n",
    "X_train_full, y_train_full, X_test = preprocess_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA AUGMENTATION =====\n",
    "def augment_data(X, y, noise_level=0.03, augment_factor=5):\n",
    "    \"\"\"Data Augmentation with Gaussian Noise\"\"\"\n",
    "    X_list = [X]\n",
    "    y_list = [y]\n",
    "    feature_std = np.std(X, axis=0)\n",
    "    \n",
    "    for i in range(augment_factor - 1):\n",
    "        noise = np.random.normal(0, noise_level, X.shape) * feature_std\n",
    "        X_noisy = X + noise\n",
    "        y_noise = np.random.normal(1.0, 0.01, y.shape)\n",
    "        y_noisy = y * y_noise\n",
    "        X_list.append(X_noisy)\n",
    "        y_list.append(y_noisy)\n",
    "    \n",
    "    return np.vstack(X_list), np.hstack(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Augmentation (nur auf Training)\n",
    "X_train_aug, y_train_aug = augment_data(X_train.values, y_train, noise_level=0.03, augment_factor=5)\n",
    "\n",
    "# Target Transformation\n",
    "y_train_log = np.log1p(y_train_aug)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train_log.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = y_scaler.transform(y_val_log.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_aug)\n",
    "X_val_scaled = scaler.transform(X_val.values)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RESIDUAL NEURAL NETWORK =====\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block with Skip Connection\"\"\"\n",
    "    def __init__(self, dim, dropout_rate=0.25):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        self.bn = nn.BatchNorm1d(dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.fc(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + identity  # Skip Connection!\n",
    "        return out\n",
    "\n",
    "class ResidualNet(nn.Module):\n",
    "    \"\"\"Deep Residual Network for Regression\"\"\"\n",
    "    def __init__(self, input_dim, dropout_rate=0.25):\n",
    "        super(ResidualNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.res1 = ResidualBlock(128, dropout_rate)\n",
    "        self.res2 = ResidualBlock(128, dropout_rate)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate * 0.7)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate * 0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.bn4 = nn.BatchNorm1d(16)\n",
    "        \n",
    "        # Output\n",
    "        self.fc_out = nn.Linear(16, 1)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc_out(x).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAINING FUNCTION =====\n",
    "def train_model(X_train, y_train, X_val, y_val, seed=42, epochs=500):\n",
    "    \"\"\"Train a single model\"\"\"\n",
    "    # Set seed\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train).to(device)\n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_t = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    # Model\n",
    "    model = ResidualNet(input_dim=X_train.shape[1], dropout_rate=0.25).to(device)\n",
    "    \n",
    "    # Huber Loss (robuster als MSE!)\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n",
    "    \n",
    "    # Early Stopping\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 50\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early Stopping\n",
    "        if val_loss < best_loss - 0.0001:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ensemble (5 models)...\n",
      "Training model 1/5 (seed=42)...\n",
      "Training model 2/5 (seed=123)...\n",
      "Training model 3/5 (seed=456)...\n",
      "Training model 4/5 (seed=789)...\n",
      "Training model 5/5 (seed=999)...\n",
      "\n",
      "Ensemble Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# ===== ENSEMBLE LEARNING - Train 5 Models =====\n",
    "print(\"Training Ensemble (5 models)...\")\n",
    "ensemble_models = []\n",
    "seeds = [42, 123, 456, 789, 999]\n",
    "\n",
    "for i, seed in enumerate(seeds, 1):\n",
    "    print(f\"Training model {i}/5 (seed={seed})...\")\n",
    "    model = train_model(X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, seed=seed)\n",
    "    ensemble_models.append(model)\n",
    "\n",
    "print(\"\\nEnsemble Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENSEMBLE PREDICTIONS =====\n",
    "def ensemble_predict(models, X_tensor):\n",
    "    \"\"\"Average predictions from all models\"\"\"\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_tensor).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "# Validation Predictions\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "val_predictions_scaled = ensemble_predict(ensemble_models, X_val_tensor)\n",
    "\n",
    "# Inverse Transform\n",
    "val_predictions_log = y_scaler.inverse_transform(val_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "val_predictions = np.expm1(val_predictions_log)\n",
    "\n",
    "y_val_log_inv = y_scaler.inverse_transform(y_val_scaled.reshape(-1, 1)).flatten()\n",
    "y_val_original = np.expm1(y_val_log_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL VALIDATION METRICS (Ensemble of 5 Models)\n",
      "============================================================\n",
      "RMSE:  $52,992.65\n",
      "MAE:   $33,557.81\n",
      "R²:    0.7922\n",
      "MAPE:  16.64%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== FINAL VALIDATION METRICS =====\n",
    "rmse = np.sqrt(mean_squared_error(y_val_original, val_predictions))\n",
    "mae = mean_absolute_error(y_val_original, val_predictions)\n",
    "r2 = r2_score(y_val_original, val_predictions)\n",
    "mape = np.mean(np.abs((y_val_original - val_predictions) / y_val_original)) * 100\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL VALIDATION METRICS (Ensemble of 5 Models)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"RMSE:  ${rmse:,.2f}\")\n",
    "print(f\"MAE:   ${mae:,.2f}\")\n",
    "print(f\"R²:    {r2:.4f}\")\n",
    "print(f\"MAPE:  {mape:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Submission saved: submission_optimized_ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== TEST PREDICTIONS =====\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "test_predictions_scaled = ensemble_predict(ensemble_models, X_test_tensor)\n",
    "\n",
    "# Inverse Transform\n",
    "test_predictions_log = y_scaler.inverse_transform(test_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "test_predictions = np.expm1(test_predictions_log)\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({\n",
    "    'Id': range(len(test_predictions)),\n",
    "    'Predicted': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_optimized_ensemble.csv', index=False)\n",
    "print(f\"\\n✓ Submission saved: submission_optimized_ensemble.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
