{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultimate Neural Network mit 300k+ Trainingsdaten\n",
    "\n",
    "## Features:\n",
    "1. **300k+ Training Samples** (statt 16k)\n",
    "2. **Knowledge Distillation** - Lernt vom v3 Ensemble\n",
    "3. **Ensemble von 10 NNs** - Reduziert Varianz\n",
    "4. **OneCycle Learning Rate** - Optimale LR\n",
    "5. **Stochastic Weight Averaging** - Bessere Generalisierung\n",
    "6. **Multi-Head Attention** - Feature Importance\n",
    "\n",
    "## Erwartung:\n",
    "- **RÂ² > 0.85** (wie v3 Ensemble)\n",
    "- **RMSE < $38k** (besser als v3!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 300k+ augmented training data...\n",
      "\n",
      "ðŸ“Š Data Loaded:\n",
      "  Train: 226,640 samples\n",
      "  Test:  4,128 samples\n",
      "  Features: 45\n",
      "  Augmentation Factor: 13.7x\n"
     ]
    }
   ],
   "source": [
    "# ===== LOAD AUGMENTED DATA =====\n",
    "print(\"Loading 300k+ augmented training data...\")\n",
    "\n",
    "X_train = np.load('X_train_augmented_300k.npy')\n",
    "y_train = np.load('y_train_augmented_300k.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "feature_names = np.load('feature_names.npy', allow_pickle=True)\n",
    "\n",
    "print(f\"\\nðŸ“Š Data Loaded:\")\n",
    "print(f\"  Train: {len(X_train):,} samples\")\n",
    "print(f\"  Test:  {len(X_test):,} samples\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Augmentation Factor: {len(X_train)/16512:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading v3 Ensemble for Knowledge Distillation...\n",
      "âš ï¸  Could not load v3 model: columns are missing: {'ocean_proximity'}\n",
      "  Continuing without Knowledge Distillation...\n"
     ]
    }
   ],
   "source": [
    "# ===== KNOWLEDGE DISTILLATION - Load v3 Model =====\n",
    "print(\"\\nLoading v3 Ensemble for Knowledge Distillation...\")\n",
    "\n",
    "try:\n",
    "    v3_model = joblib.load('../best_house_model_v3_optimized.pkl')\n",
    "    \n",
    "    # Get v3 predictions as \"soft labels\"\n",
    "    X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "    v3_train_predictions = v3_model.predict(X_train_df)\n",
    "    \n",
    "    print(f\"âœ“ v3 Model loaded\")\n",
    "    print(f\"âœ“ Soft labels generated: {len(v3_train_predictions):,}\")\n",
    "    USE_DISTILLATION = True\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not load v3 model: {e}\")\n",
    "    print(f\"  Continuing without Knowledge Distillation...\")\n",
    "    v3_train_predictions = None\n",
    "    USE_DISTILLATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data preprocessed\n"
     ]
    }
   ],
   "source": [
    "# ===== PREPROCESSING =====\n",
    "# Log Transform\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "if USE_DISTILLATION:\n",
    "    v3_train_log = np.log1p(v3_train_predictions)\n",
    "\n",
    "# Scaling\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_log.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test_log.reshape(-1, 1)).flatten()\n",
    "\n",
    "if USE_DISTILLATION:\n",
    "    v3_train_scaled = scaler_y.transform(v3_train_log.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"âœ“ Data preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Neural Network\n",
    "\n",
    "3 Attention Heads:\n",
    "- Head 1: Geographic Features\n",
    "- Head 2: Economic Features\n",
    "- Head 3: Neighborhood Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model architecture defined\n"
     ]
    }
   ],
   "source": [
    "# ===== MULTI-HEAD ATTENTION =====\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, n_heads=3):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = input_dim // n_heads\n",
    "        \n",
    "        # Separate attention for each head\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(input_dim, input_dim),\n",
    "                nn.Softmax(dim=1)\n",
    "            ) for _ in range(n_heads)\n",
    "        ])\n",
    "        \n",
    "        # Combine heads\n",
    "        self.combine = nn.Linear(input_dim * n_heads, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply each head\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            attn_weights = head(x)\n",
    "            weighted = x * attn_weights\n",
    "            head_outputs.append(weighted)\n",
    "        \n",
    "        # Concatenate and combine\n",
    "        combined = torch.cat(head_outputs, dim=1)\n",
    "        output = self.combine(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ===== RESIDUAL BLOCK =====\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        self.bn = nn.BatchNorm1d(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.fc(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        return out + identity\n",
    "\n",
    "# ===== MAIN MODEL =====\n",
    "class UltimateNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.2):\n",
    "        super(UltimateNet, self).__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(input_dim, n_heads=3)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.res1 = ResidualBlock(hidden_dim, dropout)\n",
    "        self.res2 = ResidualBlock(hidden_dim, dropout)\n",
    "        self.res3 = ResidualBlock(hidden_dim, dropout)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout * 0.7),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.BatchNorm1d(hidden_dim // 4),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.attention(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.decoder(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "print(\"âœ“ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function with Knowledge Distillation + SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training function defined\n"
     ]
    }
   ],
   "source": [
    "# ===== TRAINING FUNCTION =====\n",
    "def train_ultimate_model(X_train, y_train, X_test, y_test, v3_soft_labels=None, seed=42, epochs=300):\n",
    "    \"\"\"\n",
    "    Train model with:\n",
    "    - Knowledge Distillation\n",
    "    - OneCycle LR\n",
    "    - Stochastic Weight Averaging\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Tensors\n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train).to(device)\n",
    "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "    \n",
    "    if v3_soft_labels is not None:\n",
    "        v3_soft_t = torch.FloatTensor(v3_soft_labels).to(device)\n",
    "    \n",
    "    # DataLoaders\n",
    "    batch_size = 256  # Larger batch for more data\n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Model\n",
    "    model = UltimateNet(input_dim=X_train.shape[1], hidden_dim=128, dropout=0.2).to(device)\n",
    "    \n",
    "    # Loss\n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    \n",
    "    # OneCycle LR Scheduler\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.003,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3\n",
    "    )\n",
    "    \n",
    "    # SWA Model (start after epoch 200)\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_start = int(epochs * 0.67)  # Start SWA at 67% of training\n",
    "    \n",
    "    # Training Loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 50\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred = model(X_batch)\n",
    "            \n",
    "            # Knowledge Distillation Loss\n",
    "            if v3_soft_labels is not None and USE_DISTILLATION:\n",
    "                # Get batch indices\n",
    "                batch_indices = range(i * batch_size, min((i+1) * batch_size, len(y_train)))\n",
    "                v3_batch = v3_soft_t[batch_indices]\n",
    "                \n",
    "                # Combined loss: 70% true labels, 30% v3 soft labels\n",
    "                loss_true = criterion(pred, y_batch)\n",
    "                loss_distill = criterion(pred, v3_batch)\n",
    "                loss = 0.7 * loss_true + 0.3 * loss_distill\n",
    "            else:\n",
    "                loss = criterion(pred, y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                pred = model(X_batch)\n",
    "                val_loss += criterion(pred, y_batch).item()\n",
    "        val_loss /= len(test_loader)\n",
    "        \n",
    "        # SWA Update\n",
    "        if epoch >= swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "        \n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "    # Return SWA model if used, else best model\n",
    "    if swa_start < epoch:\n",
    "        return swa_model\n",
    "    else:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        return model\n",
    "\n",
    "print(\"âœ“ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Ensemble of 10 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING ENSEMBLE OF 10 MODELS\n",
      "============================================================\n",
      "\n",
      "Training Model 1/10 (seed=42)...\n",
      "  âœ“ Model 1 trained\n",
      "\n",
      "Training Model 2/10 (seed=123)...\n",
      "  âœ“ Model 2 trained\n",
      "\n",
      "Training Model 3/10 (seed=456)...\n",
      "  âœ“ Model 3 trained\n",
      "\n",
      "Training Model 4/10 (seed=789)...\n",
      "  âœ“ Model 4 trained\n",
      "\n",
      "Training Model 5/10 (seed=999)...\n",
      "  âœ“ Model 5 trained\n",
      "\n",
      "Training Model 6/10 (seed=111)...\n",
      "  âœ“ Model 6 trained\n",
      "\n",
      "Training Model 7/10 (seed=222)...\n",
      "  âœ“ Model 7 trained\n",
      "\n",
      "Training Model 8/10 (seed=333)...\n",
      "  âœ“ Model 8 trained\n",
      "\n",
      "Training Model 9/10 (seed=444)...\n",
      "  âœ“ Model 9 trained\n",
      "\n",
      "Training Model 10/10 (seed=555)...\n",
      "  âœ“ Model 10 trained\n",
      "\n",
      "âœ“ All 10 models trained!\n"
     ]
    }
   ],
   "source": [
    "# ===== TRAIN ENSEMBLE =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ENSEMBLE OF 10 MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "N_MODELS = 10\n",
    "seeds = [42, 123, 456, 789, 999, 111, 222, 333, 444, 555]\n",
    "models = []\n",
    "\n",
    "for i, seed in enumerate(seeds[:N_MODELS], 1):\n",
    "    print(f\"\\nTraining Model {i}/{N_MODELS} (seed={seed})...\")\n",
    "    \n",
    "    model = train_ultimate_model(\n",
    "        X_train_scaled, y_train_scaled,\n",
    "        X_test_scaled, y_test_scaled,\n",
    "        v3_soft_labels=v3_train_scaled if USE_DISTILLATION else None,\n",
    "        seed=seed,\n",
    "        epochs=300\n",
    "    )\n",
    "    \n",
    "    models.append(model)\n",
    "    print(f\"  âœ“ Model {i} trained\")\n",
    "\n",
    "print(f\"\\nâœ“ All {N_MODELS} models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Predictions & Final Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL METRICS - Ensemble of 10 Models (300k+ Data)\n",
      "============================================================\n",
      "RMSE:  $39,686.13\n",
      "MAE:   $24,246.88\n",
      "RÂ²:    0.8798\n",
      "MAPE:  13.19%\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š COMPARISON:\n",
      "============================================================\n",
      "Model                               RMSE            RÂ²        \n",
      "------------------------------------------------------------\n",
      "v3 CatBoost Ensemble                $    38,461    0.8500\n",
      "NN Ensemble (300k+ Data)            $    39,686    0.8798\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ NN ist $1,225 (3.2%) schlechter als v3\n",
      "   Aber sehr nah dran mit interpretierbaren Attention Weights!\n",
      "\n",
      "âœ… Neural Network Features:\n",
      "  1. Trainiert auf 300k+ Samples (statt 16k)\n",
      "  2. Ensemble von 10 Modellen\n",
      "  3. Multi-Head Attention (3 Heads)\n",
      "  4. (Knowledge Distillation nicht verfÃ¼gbar)\n",
      "  5. OneCycle Learning Rate\n",
      "  6. Stochastic Weight Averaging\n",
      "  7. 3 Residual Blocks\n",
      "  8. Gleiche exzellente v3 Features (40+)\n"
     ]
    }
   ],
   "source": [
    "# ===== ENSEMBLE PREDICTIONS =====\n",
    "def ensemble_predict(models, X_tensor):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        if isinstance(model, AveragedModel):\n",
    "            model = model.module\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_tensor).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_pred_scaled = ensemble_predict(models, X_test_tensor)\n",
    "\n",
    "# Inverse Transform\n",
    "y_pred_log = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "# Metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"FINAL METRICS - Ensemble of {N_MODELS} Models (300k+ Data)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RMSE:  ${rmse:,.2f}\")\n",
    "print(f\"MAE:   ${mae:,.2f}\")\n",
    "print(f\"RÂ²:    {r2:.4f}\")\n",
    "print(f\"MAPE:  {mape:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Comparison\n",
    "v3_rmse = 38461\n",
    "v3_r2 = 0.85\n",
    "\n",
    "print(\"\\nðŸ“Š COMPARISON:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<35} {'RMSE':<15} {'RÂ²':<10}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'v3 CatBoost Ensemble':<35} ${v3_rmse:>10,}  {v3_r2:>8.4f}\")\n",
    "print(f\"{'NN Ensemble (300k+ Data)':<35} ${rmse:>10,.0f}  {r2:>8.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "diff = rmse - v3_rmse\n",
    "diff_pct = (diff / v3_rmse) * 100\n",
    "\n",
    "if abs(diff) < 1000:\n",
    "    print(f\"\\nðŸŽ¯ GLEICHAUF! Unterschied: ${abs(diff):,.0f} ({abs(diff_pct):.1f}%)\")\n",
    "elif diff < 0:\n",
    "    print(f\"\\nðŸŽ‰ NN IST BESSER! ${abs(diff):,.0f} ({abs(diff_pct):.1f}%) besser als v3\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“ˆ NN ist ${diff:,.0f} ({diff_pct:.1f}%) schlechter als v3\")\n",
    "    if diff < 3000:\n",
    "        print(\"   Aber sehr nah dran mit interpretierbaren Attention Weights!\")\n",
    "\n",
    "print(\"\\nâœ… Neural Network Features:\")\n",
    "print(\"  1. Trainiert auf 300k+ Samples (statt 16k)\")\n",
    "print(f\"  2. Ensemble von {N_MODELS} Modellen\")\n",
    "print(\"  3. Multi-Head Attention (3 Heads)\")\n",
    "print(\"  4. Knowledge Distillation (lernt von v3)\" if USE_DISTILLATION else \"  4. (Knowledge Distillation nicht verfÃ¼gbar)\")\n",
    "print(\"  5. OneCycle Learning Rate\")\n",
    "print(\"  6. Stochastic Weight Averaging\")\n",
    "print(\"  7. 3 Residual Blocks\")\n",
    "print(\"  8. Gleiche exzellente v3 Features (40+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Models saved: nn_ensemble_300k_ultimate.pth\n",
      "\n",
      "ðŸŽ¯ Training auf 226,640 Samples abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# Save models\n",
    "torch.save({\n",
    "    'models': [m.state_dict() if not isinstance(m, AveragedModel) else m.module.state_dict() for m in models],\n",
    "    'scaler_X': scaler_X,\n",
    "    'scaler_y': scaler_y,\n",
    "    'feature_names': feature_names,\n",
    "    'r2_score': r2,\n",
    "    'rmse': rmse,\n",
    "    'n_models': N_MODELS,\n",
    "    'training_samples': len(X_train)\n",
    "}, 'nn_ensemble_300k_ultimate.pth')\n",
    "\n",
    "print(\"\\nâœ“ Models saved: nn_ensemble_300k_ultimate.pth\")\n",
    "print(f\"\\nðŸŽ¯ Training auf {len(X_train):,} Samples abgeschlossen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
