{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultimate Optimized NN - Target: R² > 0.82\n",
    "\n",
    "## Implementierte Verbesserungen:\n",
    "1. **Hyperparameter Optimization** (Optuna)\n",
    "2. **XGBoost + NN Hybrid Ensemble**\n",
    "3. **Stacked Meta-Ensemble**\n",
    "4. **5-Fold Cross-Validation**\n",
    "5. **Advanced Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Features: 41 | Samples: 16346\n"
     ]
    }
   ],
   "source": [
    "# ===== ADVANCED FEATURE ENGINEERING =====\n",
    "def create_advanced_features(df, fit_kmeans=None):\n",
    "    \"\"\"Ultimate Feature Engineering Pipeline\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === Basic Features ===\n",
    "    df['rooms_per_household'] = df['total_rooms'] / (df['households'] + 1)\n",
    "    df['bedrooms_ratio'] = df['total_bedrooms'] / (df['total_rooms'] + 1)\n",
    "    df['population_density'] = df['population'] / (df['households'] + 1)\n",
    "    df['income_per_bedroom'] = df['median_income'] / (df['total_bedrooms'] + 1)\n",
    "    df['rooms_per_person'] = df['total_rooms'] / (df['population'] + 1)\n",
    "    df['bedrooms_per_person'] = df['total_bedrooms'] / (df['population'] + 1)\n",
    "    \n",
    "    # === Polynomial Features (wichtigste) ===\n",
    "    df['income_squared'] = df['median_income'] ** 2\n",
    "    df['income_cubed'] = df['median_income'] ** 3\n",
    "    df['income_sqrt'] = np.sqrt(df['median_income'])\n",
    "    df['age_squared'] = df['housing_median_age'] ** 2\n",
    "    \n",
    "    # === Interaction Features ===\n",
    "    df['income_x_rooms'] = df['median_income'] * df['rooms_per_household']\n",
    "    df['income_x_age'] = df['median_income'] * df['housing_median_age']\n",
    "    df['income_x_density'] = df['median_income'] * df['population_density']\n",
    "    df['age_x_rooms'] = df['housing_median_age'] * df['rooms_per_household']\n",
    "    \n",
    "    # === Geospatial Features ===\n",
    "    df['location_interaction'] = df['longitude'] * df['latitude']\n",
    "    df['distance_to_center'] = np.sqrt(df['longitude']**2 + df['latitude']**2)\n",
    "    \n",
    "    # === Clustering Features (Geospatial Clusters) ===\n",
    "    if fit_kmeans is None:\n",
    "        kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "        df['geo_cluster'] = kmeans.fit_predict(df[['longitude', 'latitude']])\n",
    "    else:\n",
    "        df['geo_cluster'] = fit_kmeans.predict(df[['longitude', 'latitude']])\n",
    "        kmeans = fit_kmeans\n",
    "    \n",
    "    # === Log Transforms (skewed features) ===\n",
    "    df['log_population'] = np.log1p(df['population'])\n",
    "    df['log_total_rooms'] = np.log1p(df['total_rooms'])\n",
    "    \n",
    "    return df, kmeans\n",
    "\n",
    "def preprocess_data(train_df, test_df):\n",
    "    target_col = 'median_house_value'\n",
    "    X_train = train_df.drop(columns=[target_col])\n",
    "    y_train = train_df[target_col].values\n",
    "    \n",
    "    # Outlier Removal\n",
    "    lower_q = np.percentile(y_train, 1)\n",
    "    mask = y_train >= lower_q\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    \n",
    "    # Feature Engineering\n",
    "    X_train, kmeans = create_advanced_features(X_train)\n",
    "    X_test, _ = create_advanced_features(test_df.copy(), fit_kmeans=kmeans)\n",
    "    \n",
    "    # Remove IDs\n",
    "    X_train = X_train.drop(columns=['id'], errors='ignore')\n",
    "    X_test = X_test.drop(columns=['id'], errors='ignore')\n",
    "    \n",
    "    # One-Hot Encoding (ocean_proximity + geo_cluster)\n",
    "    cat_cols = X_train.select_dtypes(include=['object']).columns.tolist() + ['geo_cluster']\n",
    "    X_train = pd.get_dummies(X_train, columns=cat_cols, drop_first=False)\n",
    "    X_test = pd.get_dummies(X_test, columns=cat_cols, drop_first=False)\n",
    "    for col in set(X_train.columns) - set(X_test.columns):\n",
    "        X_test[col] = 0\n",
    "    X_test = X_test[X_train.columns]\n",
    "    \n",
    "    # Imputation\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "print(\"Loading and preprocessing data...\")\n",
    "train_df = pd.read_csv('../train.csv')\n",
    "test_df = pd.read_csv('../test.csv')\n",
    "X_full, y_full, X_test = preprocess_data(train_df, test_df)\n",
    "print(f\"Features: {X_full.shape[1]} | Samples: {len(X_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA AUGMENTATION =====\n",
    "def augment_data(X, y, noise_level=0.025, augment_factor=4):\n",
    "    X_list = [X]\n",
    "    y_list = [y]\n",
    "    feature_std = np.std(X, axis=0)\n",
    "    \n",
    "    for i in range(augment_factor - 1):\n",
    "        noise = np.random.normal(0, noise_level, X.shape) * feature_std\n",
    "        X_noisy = X + noise\n",
    "        y_noise = np.random.normal(1.0, 0.008, y.shape)\n",
    "        y_noisy = y * y_noise\n",
    "        X_list.append(X_noisy)\n",
    "        y_list.append(y_noisy)\n",
    "    \n",
    "    return np.vstack(X_list), np.hstack(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== OPTIMIZED RESIDUAL NETWORK =====\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout_rate=0.2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        self.bn = nn.BatchNorm1d(dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.fc(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        return out + identity\n",
    "\n",
    "class OptimizedResidualNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, n_residual_blocks=2, dropout_rate=0.25):\n",
    "        super(OptimizedResidualNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dim, dropout_rate) for _ in range(n_residual_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout_rate * 0.7),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.BatchNorm1d(hidden_dim // 4),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "        x = self.decoder(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: HYPERPARAMETER OPTIMIZATION\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800e8236ad544b55aec1959d33d45551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-11-20 13:52:13,313] Trial 10 failed with parameters: {'lr': 0.0014213417885061066, 'weight_decay': 0.001148776094824234, 'dropout': 0.19362508558795238, 'hidden_dim': 192, 'n_res_blocks': 2, 'batch_size': 64} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cedricstillecke/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/10/t6g6ht253j50pfcx5b145zm40000gn/T/ipykernel_46354/1431759198.py\", line 102, in <lambda>\n",
      "    lambda trial: objective(trial, X_train_scaled_hp, y_train_scaled_hp, X_val_scaled_hp, y_val_scaled_hp),\n",
      "                  ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/10/t6g6ht253j50pfcx5b145zm40000gn/T/ipykernel_46354/1431759198.py\", line 53, in objective\n",
      "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cedricstillecke/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py\", line 43, in _no_grad_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/cedricstillecke/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py\", line 232, in clip_grad_norm_\n",
      "    _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cedricstillecke/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py\", line 43, in _no_grad_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/cedricstillecke/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py\", line 165, in _clip_grads_with_norm_\n",
      "    clip_coef = max_norm / (total_norm + 1e-6)\n",
      "                ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "  File \"/Users/cedricstillecke/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/_tensor.py\", line 38, in wrapped\n",
      "    @functools.wraps(f)\n",
      "    \n",
      "KeyboardInterrupt\n",
      "[W 2025-11-20 13:52:13,318] Trial 10 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Run Optuna\u001b[39;00m\n\u001b[32m    100\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_scaled_hp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_scaled_hp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_scaled_hp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_scaled_hp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    105\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m best_params = study.best_params\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest Hyperparameters:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Run Optuna\u001b[39;00m\n\u001b[32m    100\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    101\u001b[39m study.optimize(\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_scaled_hp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_scaled_hp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_scaled_hp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_scaled_hp\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    103\u001b[39m     n_trials=\u001b[32m50\u001b[39m,\n\u001b[32m    104\u001b[39m     show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    105\u001b[39m )\n\u001b[32m    107\u001b[39m best_params = study.best_params\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest Hyperparameters:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial, X_train, y_train, X_val, y_val)\u001b[39m\n\u001b[32m     51\u001b[39m     loss = criterion(pred, y_batch)\n\u001b[32m     52\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     optimizer.step()\n\u001b[32m     56\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:43\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:232\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    230\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    231\u001b[39m total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[43m_clip_grads_with_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:43\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:165\u001b[39m, in \u001b[36m_clip_grads_with_norm_\u001b[39m\u001b[34m(parameters, max_norm, total_norm, foreach)\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    161\u001b[39m grouped_grads: \u001b[38;5;28mdict\u001b[39m[\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mtuple\u001b[39m[torch.device, torch.dtype], \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Tensor]], \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]]\n\u001b[32m    163\u001b[39m ] = _group_tensors_by_device_and_dtype([grads])  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m clip_coef = \u001b[43mmax_norm\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_norm\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# when the gradients do not reside in CPU memory.\u001b[39;00m\n\u001b[32m    169\u001b[39m clip_coef_clamped = torch.clamp(clip_coef, \u001b[38;5;28mmax\u001b[39m=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/_tensor.py:38\u001b[39m, in \u001b[36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m(\n\u001b[32m     36\u001b[39m     f: Callable[Concatenate[_TensorLike, _P], \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     37\u001b[39m ) -> Callable[Concatenate[_TensorLike, _P], \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m: _TensorLike, *args: _P.args, **kwargs: _P.kwargs) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     40\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     41\u001b[39m             \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[32m     42\u001b[39m             sargs = \u001b[38;5;28mself\u001b[39m, *args\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===== HYPERPARAMETER OPTIMIZATION WITH OPTUNA =====\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Optuna Objective Function\"\"\"\n",
    "    # Hyperparameters to optimize\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 5e-3)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-3, 5e-2)\n",
    "    dropout = trial.suggest_uniform('dropout', 0.15, 0.35)\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [96, 128, 160, 192])\n",
    "    n_res_blocks = trial.suggest_int('n_res_blocks', 1, 3)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train).to(device)\n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_t = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train_t, y_train_t), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(X_val_t, y_val_t), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = OptimizedResidualNet(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_residual_blocks=n_res_blocks,\n",
    "        dropout_rate=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30)\n",
    "    \n",
    "    # Training (reduced epochs for speed)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                pred = model(X_batch)\n",
    "                val_loss += criterion(pred, y_batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Small train/val for hyperparameter search\n",
    "X_train_hp, X_val_hp, y_train_hp, y_val_hp = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Augment\n",
    "X_train_aug_hp, y_train_aug_hp = augment_data(X_train_hp.values, y_train_hp, augment_factor=4)\n",
    "\n",
    "# Scale\n",
    "y_train_log_hp = np.log1p(y_train_aug_hp)\n",
    "y_val_log_hp = np.log1p(y_val_hp)\n",
    "y_scaler_hp = StandardScaler()\n",
    "y_train_scaled_hp = y_scaler_hp.fit_transform(y_train_log_hp.reshape(-1, 1)).flatten()\n",
    "y_val_scaled_hp = y_scaler_hp.transform(y_val_log_hp.reshape(-1, 1)).flatten()\n",
    "\n",
    "scaler_hp = StandardScaler()\n",
    "X_train_scaled_hp = scaler_hp.fit_transform(X_train_aug_hp)\n",
    "X_val_scaled_hp = scaler_hp.transform(X_val_hp)\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, X_train_scaled_hp, y_train_scaled_hp, X_val_scaled_hp, y_val_scaled_hp),\n",
    "    n_trials=50,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"Best Val Loss: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAIN NEURAL NETWORK ENSEMBLE WITH BEST PARAMS =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: TRAINING OPTIMIZED NN ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def train_optimized_model(X_train, y_train, X_val, y_val, params, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train).to(device)\n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_t = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train_t, y_train_t),\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(X_val_t, y_val_t),\n",
    "        batch_size=params['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    model = OptimizedResidualNet(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim=params['hidden_dim'],\n",
    "        n_residual_blocks=params['n_res_blocks'],\n",
    "        dropout_rate=params['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=params['lr'],\n",
    "        weight_decay=params['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_state = None\n",
    "    patience = 50\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(500):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                pred = model(X_batch)\n",
    "                val_loss += criterion(pred, y_batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_loss < best_loss - 1e-4:\n",
    "            best_loss = val_loss\n",
    "            best_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "# Full train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Augmentation\n",
    "X_train_aug, y_train_aug = augment_data(X_train.values, y_train, augment_factor=4)\n",
    "\n",
    "# Scaling\n",
    "y_train_log = np.log1p(y_train_aug)\n",
    "y_val_log = np.log1p(y_val)\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train_log.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = y_scaler.transform(y_val_log.reshape(-1, 1)).flatten()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_aug)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Ensemble\n",
    "nn_models = []\n",
    "seeds = [42, 123, 456, 789, 999]\n",
    "\n",
    "for i, seed in enumerate(seeds, 1):\n",
    "    print(f\"Training NN model {i}/5 (seed={seed})...\")\n",
    "    model = train_optimized_model(\n",
    "        X_train_scaled, y_train_scaled,\n",
    "        X_val_scaled, y_val_scaled,\n",
    "        best_params, seed\n",
    "    )\n",
    "    nn_models.append(model)\n",
    "\n",
    "print(\"NN Ensemble Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAIN LIGHTGBM MODELS =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: TRAINING LIGHTGBM ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lgb_models = []\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'max_depth': 8,\n",
    "    'min_child_samples': 20,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1\n",
    "}\n",
    "\n",
    "for i, seed in enumerate(seeds, 1):\n",
    "    print(f\"Training LightGBM model {i}/5 (seed={seed})...\")\n",
    "    lgb_params['random_state'] = seed\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, y_train)\n",
    "    val_data = lgb.Dataset(X_val, y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    lgb_models.append(model)\n",
    "\n",
    "print(\"LightGBM Ensemble Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STACKED META-ENSEMBLE =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: STACKED META-ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predictions from all models on validation set\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "nn_val_preds = []\n",
    "for model in nn_models:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_scaled = model(X_val_tensor).cpu().numpy()\n",
    "        pred_log = y_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "        pred = np.expm1(pred_log)\n",
    "        nn_val_preds.append(pred)\n",
    "\n",
    "lgb_val_preds = []\n",
    "for model in lgb_models:\n",
    "    pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    lgb_val_preds.append(pred)\n",
    "\n",
    "# Stack predictions as meta-features\n",
    "meta_features_val = np.column_stack(nn_val_preds + lgb_val_preds)\n",
    "\n",
    "# Train meta-model (Ridge Regression)\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(meta_features_val, y_val)\n",
    "\n",
    "print(f\"Meta-model weights:\")\n",
    "weights = meta_model.coef_\n",
    "for i, w in enumerate(weights):\n",
    "    model_type = \"NN\" if i < len(nn_models) else \"LGB\"\n",
    "    model_idx = (i % len(nn_models)) + 1\n",
    "    print(f\"  {model_type}-{model_idx}: {w:.4f}\")\n",
    "\n",
    "# Final prediction on validation\n",
    "val_predictions_stacked = meta_model.predict(meta_features_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FINAL VALIDATION METRICS =====\n",
    "rmse = np.sqrt(mean_squared_error(y_val, val_predictions_stacked))\n",
    "mae = mean_absolute_error(y_val, val_predictions_stacked)\n",
    "r2 = r2_score(y_val, val_predictions_stacked)\n",
    "mape = np.mean(np.abs((y_val - val_predictions_stacked) / y_val)) * 100\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL VALIDATION METRICS (Stacked Ensemble: 5xNN + 5xLGB)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"RMSE:  ${rmse:,.2f}\")\n",
    "print(f\"MAE:   ${mae:,.2f}\")\n",
    "print(f\"R²:    {r2:.4f}\")\n",
    "print(f\"MAPE:  {mape:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TEST PREDICTIONS =====\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "\n",
    "# NN predictions\n",
    "nn_test_preds = []\n",
    "for model in nn_models:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_scaled = model(X_test_tensor).cpu().numpy()\n",
    "        pred_log = y_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "        pred = np.expm1(pred_log)\n",
    "        nn_test_preds.append(pred)\n",
    "\n",
    "# LightGBM predictions\n",
    "lgb_test_preds = []\n",
    "for model in lgb_models:\n",
    "    pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    lgb_test_preds.append(pred)\n",
    "\n",
    "# Stack and predict\n",
    "meta_features_test = np.column_stack(nn_test_preds + lgb_test_preds)\n",
    "test_predictions = meta_model.predict(meta_features_test)\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({\n",
    "    'Id': range(len(test_predictions)),\n",
    "    'Predicted': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_ultimate.csv', index=False)\n",
    "print(f\"\\n✓ Submission saved: submission_ultimate.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
