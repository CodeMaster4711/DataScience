{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f581ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: MPS (Apple Silicon GPU)\n",
      "PyTorch Version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setze Random Seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# GerÃ¤teerkennung fÃ¼r Mac GPU (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"Using device: MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: CPU\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e2d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Daten...\n",
      "Train Shape: (16512, 10)\n",
      "Test Shape: (4128, 10)\n",
      "\n",
      "Erste Zeilen Train:\n",
      "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "0    -117.03     32.71                33.0       3126.0           627.0   \n",
      "1    -118.16     33.77                49.0       3382.0           787.0   \n",
      "2    -120.48     34.66                 4.0       1897.0           331.0   \n",
      "3    -117.11     32.69                36.0       1421.0           367.0   \n",
      "4    -119.80     36.78                43.0       2382.0           431.0   \n",
      "\n",
      "   population  households  median_income  median_house_value ocean_proximity  \n",
      "0      2300.0       623.0         3.2596            103000.0      NEAR OCEAN  \n",
      "1      1314.0       756.0         3.8125            382100.0      NEAR OCEAN  \n",
      "2       915.0       336.0         4.1563            172600.0      NEAR OCEAN  \n",
      "3      1418.0       355.0         1.9425             93400.0      NEAR OCEAN  \n",
      "4       874.0       380.0         3.5542             96500.0          INLAND  \n",
      "\n",
      "Spalten:\n",
      "['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'ocean_proximity']\n",
      "\n",
      "Datentypen:\n",
      "longitude             float64\n",
      "latitude              float64\n",
      "housing_median_age    float64\n",
      "total_rooms           float64\n",
      "total_bedrooms        float64\n",
      "population            float64\n",
      "households            float64\n",
      "median_income         float64\n",
      "median_house_value    float64\n",
      "ocean_proximity        object\n",
      "dtype: object\n",
      "\n",
      "Fehlende Werte:\n",
      "longitude             0\n",
      "latitude              0\n",
      "housing_median_age    0\n",
      "total_rooms           0\n",
      "total_bedrooms        0\n",
      "population            0\n",
      "households            0\n",
      "median_income         0\n",
      "median_house_value    0\n",
      "ocean_proximity       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_path = Path('../train.csv')\n",
    "test_path = Path('../test.csv')\n",
    "\n",
    "print(\"Lade Daten...\")\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Train Shape: {train_df.shape}\")\n",
    "print(f\"Test Shape: {test_df.shape}\")\n",
    "print(\"\\nErste Zeilen Train:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nSpalten:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(\"\\nDatentypen:\")\n",
    "print(train_df.dtypes)\n",
    "print(\"\\nFehlende Werte:\")\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd60516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUANTIL-BASIERTE AUSREISSER-ENTFERNUNG\n",
      "============================================================\n",
      "Original Train-GrÃ¶ÃŸe: 16512\n",
      "Untere Grenze (1% Quantil): $50,011\n",
      "Max Preis im Dataset: $500,001\n",
      "Preisverteilung:\n",
      "  - 25% Quantil: $119,800\n",
      "  - 50% Quantil: $179,850\n",
      "  - 75% Quantil: $265,125\n",
      "  - 99% Quantil: $500,001\n",
      "\n",
      "Entfernte AusreiÃŸer: 166 (1.01%)\n",
      "Neue Train-GrÃ¶ÃŸe: 16346\n",
      "Neuer Max Preis: $500,001\n",
      "âœ… Alle teuren HÃ¤user bleiben erhalten (kein Ceiling!)\n",
      "\n",
      "============================================================\n",
      "LOG-TRANSFORM (VOR FEATURE ENGINEERING!)\n",
      "============================================================\n",
      "âœ… Log-Transform angewendet BEVOR Feature Engineering\n",
      "   Original Max: $500,001\n",
      "   Log Max: 13.12\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING\n",
      "============================================================\n",
      "âœ… 4 neue Features erstellt:\n",
      "  - rooms_per_household\n",
      "  - bedrooms_ratio\n",
      "  - population_density\n",
      "  - income_per_bedroom\n",
      "\n",
      "âŒ price_per_room ENTFERNT (Test-Daten haben keinen Target!)\n",
      "\n",
      "Categorical Features: ['ocean_proximity']\n",
      "Numerical Features: 12\n",
      "Features nach Encoding: 16\n",
      "\n",
      "Train Set: (13076, 16)\n",
      "Validation Set: (3270, 16)\n",
      "Test Set: (4128, 16)\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ BUG FIXES ANGEWENDET:\n",
      "============================================================\n",
      "1. âœ… Log-Transform VOR Feature Engineering\n",
      "2. âœ… price_per_room entfernt (Test-Daten-Inkonsistenz)\n",
      "3. âœ… Target NUR log-transformiert (kein StandardScaler!)\n",
      "============================================================\n",
      "âš ï¸  WICHTIG: Target ist LOG-transformiert! Bei Predictions expm1() verwenden!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Target separieren\n",
    "target_col = 'median_house_value'\n",
    "X_train_full = train_df.drop(columns=[target_col])\n",
    "y_train_full = train_df[target_col].values\n",
    "\n",
    "# ============================================\n",
    "# ðŸš€ LÃ–SUNG 1: QUANTIL-BASIERTE AUSREISSER-ENTFERNUNG\n",
    "# WICHTIG: Nur untere AusreiÃŸer entfernen, ALLE teuren HÃ¤user behalten!\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"QUANTIL-BASIERTE AUSREISSER-ENTFERNUNG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Nur unterste 1% entfernen (sehr billige Fehldaten)\n",
    "# KEINE obere Grenze! â†’ Verhindert Ceiling Effect\n",
    "lower_quantile = np.percentile(y_train_full, 1)  # Nur unterste 1%\n",
    "\n",
    "print(f\"Original Train-GrÃ¶ÃŸe: {len(y_train_full)}\")\n",
    "print(f\"Untere Grenze (1% Quantil): ${lower_quantile:,.0f}\")\n",
    "print(f\"Max Preis im Dataset: ${y_train_full.max():,.0f}\")\n",
    "print(f\"Preisverteilung:\")\n",
    "print(f\"  - 25% Quantil: ${np.percentile(y_train_full, 25):,.0f}\")\n",
    "print(f\"  - 50% Quantil: ${np.percentile(y_train_full, 50):,.0f}\")\n",
    "print(f\"  - 75% Quantil: ${np.percentile(y_train_full, 75):,.0f}\")\n",
    "print(f\"  - 99% Quantil: ${np.percentile(y_train_full, 99):,.0f}\")\n",
    "\n",
    "# Entferne nur untere AusreiÃŸer\n",
    "outlier_mask = y_train_full >= lower_quantile  # KEINE obere Grenze!\n",
    "n_outliers = (~outlier_mask).sum()\n",
    "\n",
    "print(f\"\\nEntfernte AusreiÃŸer: {n_outliers} ({n_outliers/len(y_train_full)*100:.2f}%)\")\n",
    "\n",
    "# Entferne AusreiÃŸer\n",
    "X_train_full = X_train_full[outlier_mask]\n",
    "y_train_full = y_train_full[outlier_mask]\n",
    "\n",
    "print(f\"Neue Train-GrÃ¶ÃŸe: {len(X_train_full)}\")\n",
    "print(f\"Neuer Max Preis: ${y_train_full.max():,.0f}\")\n",
    "print(\"âœ… Alle teuren HÃ¤user bleiben erhalten (kein Ceiling!)\")\n",
    "\n",
    "# ============================================\n",
    "# ðŸ”¥ FIX BUG 1: LOG-TRANSFORM VOR FEATURE ENGINEERING!\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOG-TRANSFORM (VOR FEATURE ENGINEERING!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LOG-TRANSFORM ZUERST anwenden!\n",
    "y_train_full_log = np.log1p(y_train_full)\n",
    "print(f\"âœ… Log-Transform angewendet BEVOR Feature Engineering\")\n",
    "print(f\"   Original Max: ${y_train_full.max():,.0f}\")\n",
    "print(f\"   Log Max: {y_train_full_log.max():.2f}\")\n",
    "\n",
    "# ============================================\n",
    "# ðŸš€ LÃ–SUNG 2: FEATURE ENGINEERING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Neue Features erstellen (OHNE price_per_room!)\n",
    "# price_per_room entfernt, da Test-Daten keinen Target haben!\n",
    "\n",
    "# 1. Rooms per household (WohnungsgrÃ¶ÃŸe-Indikator)\n",
    "X_train_full['rooms_per_household'] = X_train_full['total_rooms'] / (X_train_full['households'] + 1)\n",
    "\n",
    "# 2. Bedrooms ratio (Wohnungstyp-Indikator)\n",
    "X_train_full['bedrooms_ratio'] = X_train_full['total_bedrooms'] / (X_train_full['total_rooms'] + 1)\n",
    "\n",
    "# 3. Population density (Urban vs Rural)\n",
    "X_train_full['population_density'] = X_train_full['population'] / (X_train_full['households'] + 1)\n",
    "\n",
    "# 4. Income per bedroom (Wohlstands-Indikator)\n",
    "X_train_full['income_per_bedroom'] = X_train_full['median_income'] / (X_train_full['total_bedrooms'] + 1)\n",
    "\n",
    "print(f\"âœ… 4 neue Features erstellt:\")\n",
    "print(\"  - rooms_per_household\")\n",
    "print(\"  - bedrooms_ratio\")\n",
    "print(\"  - population_density\")\n",
    "print(\"  - income_per_bedroom\")\n",
    "print(\"\\nâŒ price_per_room ENTFERNT (Test-Daten haben keinen Target!)\")\n",
    "\n",
    "# Update y_train_full zu log-transformierter Version\n",
    "y_train_full = y_train_full_log\n",
    "\n",
    "# IDs speichern falls vorhanden\n",
    "id_cols = ['id'] if 'id' in X_train_full.columns else []\n",
    "test_ids = test_df[id_cols].copy() if id_cols else None\n",
    "\n",
    "# IDs entfernen\n",
    "X_train_full = X_train_full.drop(columns=id_cols, errors='ignore')\n",
    "X_test = test_df.drop(columns=id_cols, errors='ignore')\n",
    "\n",
    "# Feature Engineering auch auf Test-Daten (OHNE price_per_room!)\n",
    "X_test['rooms_per_household'] = X_test['total_rooms'] / (X_test['households'] + 1)\n",
    "X_test['bedrooms_ratio'] = X_test['total_bedrooms'] / (X_test['total_rooms'] + 1)\n",
    "X_test['population_density'] = X_test['population'] / (X_test['households'] + 1)\n",
    "X_test['income_per_bedroom'] = X_test['median_income'] / (X_test['total_bedrooms'] + 1)\n",
    "\n",
    "# Categorical Features (ocean_proximity)\n",
    "categorical_cols = X_train_full.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_train_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical Features: {categorical_cols}\")\n",
    "print(f\"Numerical Features: {len(numerical_cols)}\")\n",
    "\n",
    "# One-Hot Encoding fÃ¼r kategorische Features\n",
    "if categorical_cols:\n",
    "    X_train_full = pd.get_dummies(X_train_full, columns=categorical_cols, drop_first=True)\n",
    "    X_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Stelle sicher, dass Train und Test die gleichen Spalten haben\n",
    "    missing_cols = set(X_train_full.columns) - set(X_test.columns)\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0\n",
    "    X_test = X_test[X_train_full.columns]\n",
    "\n",
    "print(f\"Features nach Encoding: {X_train_full.shape[1]}\")\n",
    "\n",
    "# Fehlende Werte behandeln\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_full = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train_full),\n",
    "    columns=X_train_full.columns\n",
    ")\n",
    "X_test = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=X_test.columns\n",
    ")\n",
    "\n",
    "# Train/Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain Set: {X_train.shape}\")\n",
    "print(f\"Validation Set: {X_val.shape}\")\n",
    "print(f\"Test Set: {X_test.shape}\")\n",
    "\n",
    "# Skalierung NUR fÃ¼r Features (NICHT fÃ¼r Target!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ”¥ BUG FIXES ANGEWENDET:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. âœ… Log-Transform VOR Feature Engineering\")\n",
    "print(\"2. âœ… price_per_room entfernt (Test-Daten-Inkonsistenz)\")\n",
    "print(\"3. âœ… Target NUR log-transformiert (kein StandardScaler!)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âš ï¸  WICHTIG: Target ist LOG-transformiert! Bei Predictions expm1() verwenden!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65681001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SCHRITT 4: PyTorch DataLoaders erstellen\n",
      "==================================================\n",
      "âœ… Target NICHT mit StandardScaler skaliert (nur log-transform!)\n",
      "Batch Size: 64\n",
      "Training Batches: 205\n",
      "Validation Batches: 52\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SCHRITT 4: PyTorch DataLoaders erstellen\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Zu Tensoren konvertieren (Target ist bereits log-transformiert!)\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)  # Direkt ohne StandardScaler!\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).to(device)  # Direkt ohne StandardScaler!\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "\n",
    "print(\"âœ… Target NICHT mit StandardScaler skaliert (nur log-transform!)\")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Training Batches: {len(train_loader)}\")\n",
    "print(f\"Validation Batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94fe982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SCHRITT 5: Neural Network definieren\n",
      "==================================================\n",
      "âœ… Tieferes Netzwerk: [512, 256, 128, 64, 32]\n",
      "ðŸ”¥ Dropout Rate: 0.2 (REDUZIERT von 0.5 â†’ NN darf lernen!)\n",
      "HousingPriceNN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.2, inplace=False)\n",
      "    (16): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (17): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): Dropout(p=0.2, inplace=False)\n",
      "    (20): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Anzahl Parameter: 185,281\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SCHRITT 5: Neural Network definieren\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class HousingPriceNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[256, 128, 64], dropout_rate=0.3):\n",
    "        super(HousingPriceNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Hidden Layers mit BatchNorm und Dropout\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output Layer\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Weight Initialization\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "# Model erstellen - REDUZIERTE REGULARISIERUNG!\n",
    "input_size = X_train_scaled.shape[1]\n",
    "model = HousingPriceNN(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=[512, 256, 128, 64, 32],\n",
    "    dropout_rate=0.2  # ðŸ”¥ FIX: Reduziert von 0.5 auf 0.2 (zu starke Reg!)\n",
    ").to(device)\n",
    "\n",
    "print(\"âœ… Tieferes Netzwerk: [512, 256, 128, 64, 32]\")\n",
    "print(\"ðŸ”¥ Dropout Rate: 0.2 (REDUZIERT von 0.5 â†’ NN darf lernen!)\")\n",
    "print(model)\n",
    "print(f\"\\nAnzahl Parameter: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d612159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SCHRITT 6: Training Setup\n",
      "==================================================\n",
      "ðŸ”¥ Weighted MSE Loss FIX:\n",
      "   - Threshold: 12.9 (â‰ˆ $400k) - NUR die teuersten HÃ¤user!\n",
      "   - Gewicht: 10.0x (statt 3.0x) - VIEL STÃ„RKER!\n",
      "   - MSE statt Huber â†’ GroÃŸe Fehler werden STÃ„RKER bestraft!\n",
      "   â†’ Nur TOP 5-10% der teuersten HÃ¤user bekommen Extra-Gewicht\n",
      "\n",
      "âœ… Learning Rate: 0.001 (erhÃ¶ht)\n",
      "âœ… Weight Decay: 0.005 (STARK reduziert von 0.05!)\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ CEILING EFFECT FIXES V2:\n",
      "============================================================\n",
      "1. âœ… Weighted MSE: Threshold 12.9 ($400k), Weight 10x\n",
      "2. âœ… MSE statt Huber (bestraft groÃŸe Fehler STÃ„RKER!)\n",
      "3. âœ… Weight Decay 0.05 â†’ 0.005 (10x schwÃ¤cher!)\n",
      "4. âœ… Nur TOP 5-10% teuerste HÃ¤user extra gewichtet\n",
      "============================================================\n",
      "â†’ NN darf jetzt hohe Werte vorhersagen!\n",
      "â†’ Fehler bei teuren HÃ¤usern werden STARK bestraft!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SCHRITT 6: Training Setup\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ============================================\n",
    "# ðŸ”¥ FIX: WEIGHTED MSE LOSS (RICHTIG KONFIGURIERT!)\n",
    "# ============================================\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Loss: MSE Loss mit STARKER Gewichtung fÃ¼r teure HÃ¤user\n",
    "    Threshold KORREKT gesetzt fÃ¼r teure HÃ¤user!\n",
    "    \"\"\"\n",
    "    def __init__(self, expensive_threshold=12.9, weight_multiplier=10.0):\n",
    "        super().__init__()\n",
    "        # log1p($400k) â‰ˆ 12.90 â†’ Nur TOP 5-10% der teuersten HÃ¤user!\n",
    "        self.expensive_threshold = expensive_threshold\n",
    "        self.weight_multiplier = weight_multiplier\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Berechne MSE fÃ¼r jeden Sample\n",
    "        losses = self.mse(predictions, targets)\n",
    "        \n",
    "        # Gewichte: SEHR teure HÃ¤user (> $400k) bekommen 10x Gewicht!\n",
    "        weights = torch.where(\n",
    "            targets > self.expensive_threshold,\n",
    "            torch.tensor(self.weight_multiplier, device=targets.device),\n",
    "            torch.tensor(1.0, device=targets.device)\n",
    "        )\n",
    "        \n",
    "        # Gewichteter Loss\n",
    "        weighted_loss = (weights * losses).mean()\n",
    "        return weighted_loss\n",
    "\n",
    "criterion = WeightedMSELoss(expensive_threshold=12.9, weight_multiplier=10.0)\n",
    "print(\"ðŸ”¥ Weighted MSE Loss FIX:\")\n",
    "print(f\"   - Threshold: 12.9 (â‰ˆ $400k) - NUR die teuersten HÃ¤user!\")\n",
    "print(f\"   - Gewicht: 10.0x (statt 3.0x) - VIEL STÃ„RKER!\")\n",
    "print(f\"   - MSE statt Huber â†’ GroÃŸe Fehler werden STÃ„RKER bestraft!\")\n",
    "print(f\"   â†’ Nur TOP 5-10% der teuersten HÃ¤user bekommen Extra-Gewicht\")\n",
    "\n",
    "# ðŸ”¥ Regularisierung REDUZIEREN (war zu stark!)\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.001,  # Etwas hÃ¶her fÃ¼r besseres Lernen\n",
    "    weight_decay=0.005  # VIEL schwÃ¤cher (vorher 0.05!)\n",
    ")\n",
    "print(\"\\nâœ… Learning Rate: 0.001 (erhÃ¶ht)\")\n",
    "print(\"âœ… Weight Decay: 0.005 (STARK reduziert von 0.05!)\")\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10\n",
    ")\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=20, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = model.state_dict().copy()\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = model.state_dict().copy()\n",
    "            self.counter = 0\n",
    "\n",
    "early_stopping = EarlyStopping(patience=25, min_delta=0.0001)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ”¥ CEILING EFFECT FIXES V2:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. âœ… Weighted MSE: Threshold 12.9 ($400k), Weight 10x\")\n",
    "print(\"2. âœ… MSE statt Huber (bestraft groÃŸe Fehler STÃ„RKER!)\")\n",
    "print(\"3. âœ… Weight Decay 0.05 â†’ 0.005 (10x schwÃ¤cher!)\")\n",
    "print(\"4. âœ… Nur TOP 5-10% teuerste HÃ¤user extra gewichtet\")\n",
    "print(\"=\" * 60)\n",
    "print(\"â†’ NN darf jetzt hohe Werte vorhersagen!\")\n",
    "print(\"â†’ Fehler bei teuren HÃ¤usern werden STARK bestraft!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "731cf011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SCHRITT 7: Training starten\n",
      "==================================================\n",
      "Training fÃ¼r 200 Epochen...\n",
      "Device: mps\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     val_loss = validate(model, val_loader, criterion)\n\u001b[32m     43\u001b[39m     train_losses.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer)\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Gradient Clipping\u001b[39;00m\n\u001b[32m     15\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     total_loss += loss.item()\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/optim/adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CloudExplain/DataScienceTutorial/.venv/lib/python3.13/site-packages/torch/optim/adam.py:411\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight_decay != \u001b[32m0\u001b[39m:\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoupled_weight_decay:\n\u001b[32m    410\u001b[39m         \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m         \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    413\u001b[39m         \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[32m    414\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weight_decay, Tensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SCHRITT 7: Training starten\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(f\"Training fÃ¼r {num_epochs} Epochen...\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning Rate anpassen\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early Stopping prÃ¼fen\n",
    "    early_stopping(val_loss, model)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {train_loss:.6f} - \"\n",
    "              f\"Val Loss: {val_loss:.6f} - \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\nEarly Stopping bei Epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Bestes Model laden\n",
    "model.load_state_dict(early_stopping.best_model)\n",
    "print(\"\\nBestes Model geladen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SCHRITT 8: Model Evaluation\n",
      "==================================================\n",
      "\n",
      "Validation Metriken (Original-Skala):\n",
      "RMSE: $4,681,091.44\n",
      "MAE: $124,426.30\n",
      "RÂ² Score: -1653.7688\n",
      "MAPE: 38.28%\n",
      "\n",
      "âœ… Inverse Transform: expm1() only (kein StandardScaler!)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SCHRITT 8: Model Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Vorhersagen auf Validation Set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_predictions_log = model(X_val_tensor).cpu().numpy()\n",
    "\n",
    "# ðŸ”¥ FIX: Nur expm1() verwenden (kein target_scaler mehr!)\n",
    "val_predictions = np.expm1(val_predictions_log)\n",
    "y_val_original = np.expm1(y_val)  # y_val ist bereits log-transformiert\n",
    "\n",
    "# Metriken berechnen\n",
    "mse = mean_squared_error(y_val_original, val_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_val_original, val_predictions)\n",
    "r2 = r2_score(y_val_original, val_predictions)\n",
    "\n",
    "print(f\"\\nValidation Metriken (Original-Skala):\")\n",
    "print(f\"RMSE: ${rmse:,.2f}\")\n",
    "print(f\"MAE: ${mae:,.2f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "print(f\"MAPE: {np.mean(np.abs((y_val_original - val_predictions) / y_val_original)) * 100:.2f}%\")\n",
    "print(\"\\nâœ… Inverse Transform: expm1() only (kein StandardScaler!)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
