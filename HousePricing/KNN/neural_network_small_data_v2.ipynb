{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network v2 - Mit Early Stopping & Regularisierung\n",
    "## Overfitting reduzieren durch intelligentes Training\n",
    "\n",
    "In dieser Version implementieren wir:\n",
    "- **Early Stopping**: Stoppt Training automatisch, wenn Validation Loss nicht mehr besser wird\n",
    "- **Dropout**: Regularisierung durch zufÃ¤lliges Deaktivieren von Neuronen\n",
    "- **L2 Regularization (Weight Decay)**: Bestrafung groÃŸer Gewichte\n",
    "- **Learning Rate Scheduling**: Anpassung der Learning Rate wÃ¤hrend des Trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch Version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setze Random Seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daten laden (100 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kompletter Datensatz: 20640 Zeilen\n",
      "Reduzierter Datensatz: 100 Zeilen\n",
      "\n",
      "Training: 80 | Test: 20\n",
      "Features: 15\n"
     ]
    }
   ],
   "source": [
    "# Lade Datensatz\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_PATH = BASE_DIR / \"housing.csv\"\n",
    "\n",
    "df_full = pd.read_csv(DATA_PATH)\n",
    "print(f\"Kompletter Datensatz: {len(df_full)} Zeilen\")\n",
    "\n",
    "# Reduziere auf 100 Samples (stratifiziert)\n",
    "df_full['price_quartile'] = pd.qcut(df_full['median_house_value'], q=4, labels=False)\n",
    "df_small = df_full.groupby('price_quartile', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=25, random_state=42)\n",
    ").drop('price_quartile', axis=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"Reduzierter Datensatz: {len(df_small)} Zeilen\")\n",
    "\n",
    "# Feature Engineering\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    df['households'] = df['households'].replace(0, np.nan)\n",
    "    df['total_rooms'] = df['total_rooms'].replace(0, np.nan)\n",
    "    df['rooms_per_household'] = df['total_rooms'] / df['households']\n",
    "    df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']\n",
    "    df['population_per_household'] = df['population'] / df['households']\n",
    "    return df\n",
    "\n",
    "df_small = engineer_features(df_small)\n",
    "df_small = pd.get_dummies(df_small, columns=['ocean_proximity'], drop_first=False)\n",
    "\n",
    "# Train-Test Split\n",
    "X = df_small.drop('median_house_value', axis=1)\n",
    "y = df_small['median_house_value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining: {len(X_train)} | Test: {len(X_test)}\")\n",
    "\n",
    "# Preprocessing\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# PyTorch Tensoren\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train_scaled).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test_scaled).to(device)\n",
    "\n",
    "print(f\"Features: {X_train_tensor.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelle MIT Regularisierung (Dropout)\n",
    "\n",
    "Dropout deaktiviert zufÃ¤llig Neuronen wÃ¤hrend des Trainings, um Overfitting zu verhindern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelle MIT Regularisierung:\n",
      "============================================================\n",
      "Small Net  - Dropout: 20%\n",
      "Medium Net - Dropout: 30%\n",
      "Large Net  - Dropout: 40%\n"
     ]
    }
   ],
   "source": [
    "class RegularizedSmallNet(nn.Module):\n",
    "    \"\"\"Small Model mit Dropout\"\"\"\n",
    "    def __init__(self, input_dim, dropout_rate=0.2):\n",
    "        super(RegularizedSmallNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class RegularizedMediumNet(nn.Module):\n",
    "    \"\"\"Medium Model mit Dropout\"\"\"\n",
    "    def __init__(self, input_dim, dropout_rate=0.3):\n",
    "        super(RegularizedMediumNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class RegularizedLargeNet(nn.Module):\n",
    "    \"\"\"Large Model mit Dropout\"\"\"\n",
    "    def __init__(self, input_dim, dropout_rate=0.4):\n",
    "        super(RegularizedLargeNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "\n",
    "print(\"Modelle MIT Regularisierung:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Small Net  - Dropout: 20%\")\n",
    "print(f\"Medium Net - Dropout: 30%\")\n",
    "print(f\"Large Net  - Dropout: 40%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early Stopping Klasse\n",
    "\n",
    "Stoppt das Training automatisch, wenn der Validation Loss nicht mehr sinkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Klasse erstellt!\n",
      "Stoppt Training wenn Validation Loss 20 Epochen nicht besser wird.\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early Stopping um Training zu stoppen wenn Validation Loss nicht mehr sinkt\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=20, min_delta=0.001, verbose=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Wie viele Epochen ohne Verbesserung abgewartet werden\n",
    "            min_delta: Minimale Ã„nderung, die als Verbesserung gilt\n",
    "            verbose: Ausgabe von Meldungen\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def __call__(self, val_loss, epoch):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose and self.counter % 5 == 0:\n",
    "                print(f\"    Early Stopping Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(f\"\\n    âš ï¸  Early Stopping bei Epoch {epoch}!\")\n",
    "                    print(f\"    Bester Validation Loss bei Epoch {self.best_epoch}: {self.best_loss:.4f}\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            \n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "\n",
    "print(\"Early Stopping Klasse erstellt!\")\n",
    "print(\"Stoppt Training wenn Validation Loss 20 Epochen nicht besser wird.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training mit Early Stopping & Weight Decay (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Funktion mit Early Stopping & L2 Regularization bereit!\n"
     ]
    }
   ],
   "source": [
    "def train_model_with_early_stopping(\n",
    "    model, X_train, y_train, X_test, y_test, \n",
    "    max_epochs=500, \n",
    "    batch_size=16, \n",
    "    lr=0.001,\n",
    "    weight_decay=0.01,  # L2 Regularization\n",
    "    patience=20\n",
    "):\n",
    "    \"\"\"Training mit Early Stopping und L2 Regularization\"\"\"\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning Rate Scheduler: Reduziert LR wenn kein Fortschritt\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10, verbose=False\n",
    "    )\n",
    "    \n",
    "    # DataLoader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            outputs = model(batch_X).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test).squeeze()\n",
    "            val_loss = criterion(val_outputs, y_test).item()\n",
    "        \n",
    "        # Learning Rate anpassen\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Ausgabe alle 50 Epochen\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{max_epochs}] - Train: {avg_train_loss:.4f}, Val: {val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Early Stopping Check\n",
    "        early_stopping(val_loss, epoch + 1)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "    \n",
    "    final_epoch = epoch + 1\n",
    "    print(f\"\\nâœ“ Training beendet nach {final_epoch} Epochen\")\n",
    "    \n",
    "    return history, final_epoch\n",
    "\n",
    "print(\"Training-Funktion mit Early Stopping & L2 Regularization bereit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trainiere regularisierte Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING: Small Model (mit Dropout 20% + L2 + Early Stopping)\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m     12\u001b[39m model_small_reg = RegularizedSmallNet(input_dim, dropout_rate=\u001b[32m0.2\u001b[39m).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m history_small_reg, epochs_small = \u001b[43mtrain_model_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_small_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPATIENCE\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTRAINING: Medium Model (mit Dropout 30\u001b[39m\u001b[33m%\u001b[39m\u001b[33m + L2 + Early Stopping)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_model_with_early_stopping\u001b[39m\u001b[34m(model, X_train, y_train, X_test, y_test, max_epochs, batch_size, lr, weight_decay, patience)\u001b[39m\n\u001b[32m     12\u001b[39m optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Learning Rate Scheduler: Reduziert LR wenn kein Fortschritt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m scheduler = \u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# DataLoader\u001b[39;00m\n\u001b[32m     20\u001b[39m train_dataset = TensorDataset(X_train, y_train)\n",
      "\u001b[31mTypeError\u001b[39m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "MAX_EPOCHS = 500\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.01  # L2 Regularization\n",
    "PATIENCE = 20  # Early Stopping Patience\n",
    "\n",
    "# Trainiere Small Model\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING: Small Model (mit Dropout 20% + L2 + Early Stopping)\")\n",
    "print(\"=\" * 70)\n",
    "model_small_reg = RegularizedSmallNet(input_dim, dropout_rate=0.2).to(device)\n",
    "history_small_reg, epochs_small = train_model_with_early_stopping(\n",
    "    model_small_reg, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor,\n",
    "    max_epochs=MAX_EPOCHS, batch_size=BATCH_SIZE, lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY, patience=PATIENCE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING: Medium Model (mit Dropout 30% + L2 + Early Stopping)\")\n",
    "print(\"=\" * 70)\n",
    "model_medium_reg = RegularizedMediumNet(input_dim, dropout_rate=0.3).to(device)\n",
    "history_medium_reg, epochs_medium = train_model_with_early_stopping(\n",
    "    model_medium_reg, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor,\n",
    "    max_epochs=MAX_EPOCHS, batch_size=BATCH_SIZE, lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY, patience=PATIENCE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING: Large Model (mit Dropout 40% + L2 + Early Stopping)\")\n",
    "print(\"=\" * 70)\n",
    "model_large_reg = RegularizedLargeNet(input_dim, dropout_rate=0.4).to(device)\n",
    "history_large_reg, epochs_large = train_model_with_early_stopping(\n",
    "    model_large_reg, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor,\n",
    "    max_epochs=MAX_EPOCHS, batch_size=BATCH_SIZE, lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY, patience=PATIENCE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALLE MODELLE TRAINIERT!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Small Model:  Gestoppt nach {epochs_small} Epochen\")\n",
    "print(f\"Medium Model: Gestoppt nach {epochs_medium} Epochen\")\n",
    "print(f\"Large Model:  Gestoppt nach {epochs_large} Epochen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisierung: Loss Curves mit Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "histories = [\n",
    "    (history_small_reg, epochs_small, 'Small Model (Reg)', 'blue'),\n",
    "    (history_medium_reg, epochs_medium, 'Medium Model (Reg)', 'green'),\n",
    "    (history_large_reg, epochs_large, 'Large Model (Reg)', 'red')\n",
    "]\n",
    "\n",
    "for idx, (history, final_epoch, title, color) in enumerate(histories):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    ax.plot(epochs_range, history['train_loss'], label='Training Loss', \n",
    "            color=color, alpha=0.8, linewidth=2)\n",
    "    ax.plot(epochs_range, history['val_loss'], label='Validation Loss', \n",
    "            color=color, linestyle='--', linewidth=2.5)\n",
    "    \n",
    "    # Markiere Stopping Point\n",
    "    ax.axvline(x=final_epoch, color='red', linestyle=':', linewidth=2, alpha=0.7,\n",
    "               label=f'Stopped at {final_epoch}')\n",
    "    \n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "    ax.set_title(f'{title}\\nEarly Stopped', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Finale Werte\n",
    "    final_train_loss = history['train_loss'][-1]\n",
    "    final_val_loss = history['val_loss'][-1]\n",
    "    best_val_loss = min(history['val_loss'])\n",
    "    \n",
    "    ax.text(0.98, 0.98, \n",
    "            f'Final Train: {final_train_loss:.4f}\\nFinal Val: {final_val_loss:.4f}\\nBest Val: {best_val_loss:.4f}',\n",
    "            transform=ax.transAxes,\n",
    "            verticalalignment='top',\n",
    "            horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7),\n",
    "            fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance-Analyse: Regularisierte Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_reg = {\n",
    "    'Small (Reg)': model_small_reg,\n",
    "    'Medium (Reg)': model_medium_reg,\n",
    "    'Large (Reg)': model_large_reg\n",
    "}\n",
    "\n",
    "results_reg = []\n",
    "\n",
    "for name, model in models_reg.items():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(X_train_tensor).squeeze().cpu().numpy()\n",
    "        y_test_pred_scaled = model(X_test_tensor).squeeze().cpu().numpy()\n",
    "    \n",
    "    y_train_pred = y_scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test_pred = y_scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    rmse_gap = test_rmse - train_rmse\n",
    "    rmse_gap_percent = (rmse_gap / train_rmse) * 100\n",
    "    \n",
    "    results_reg.append({\n",
    "        'Model': name,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'RMSE Gap': rmse_gap,\n",
    "        'Gap %': rmse_gap_percent,\n",
    "        'Train MAE': train_mae,\n",
    "        'Test MAE': test_mae,\n",
    "        'Train RÂ²': train_r2,\n",
    "        'Test RÂ²': test_r2\n",
    "    })\n",
    "\n",
    "results_reg_df = pd.DataFrame(results_reg)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PERFORMANCE: Regularisierte Modelle (Dropout + L2 + Early Stopping)\")\n",
    "print(\"=\" * 100)\n",
    "print(results_reg_df.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Overfitting-Analyse: Regularisiert vs. Original\n",
    "\n",
    "Vergleichen wir die Overfitting Gaps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_overfitting_v2(train_rmse, test_rmse, model_name):\n",
    "    gap = test_rmse - train_rmse\n",
    "    gap_percent = (gap / train_rmse) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ANALYSE: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Training RMSE:    ${train_rmse:>10,.2f}\")\n",
    "    print(f\"Test RMSE:        ${test_rmse:>10,.2f}\")\n",
    "    print(f\"Gap:              ${gap:>10,.2f} ({gap_percent:>6.1f}%)\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    if gap_percent > 30:\n",
    "        verdict = \"ðŸ”´ STARKES OVERFITTING\"\n",
    "        status = \"Kritisch\"\n",
    "    elif gap_percent > 15:\n",
    "        verdict = \"ðŸŸ¡ MODERATES OVERFITTING\"\n",
    "        status = \"VerbesserungswÃ¼rdig\"\n",
    "    elif gap_percent > -5:\n",
    "        verdict = \"ðŸŸ¢ GUTER FIT\"\n",
    "        status = \"Optimal!\"\n",
    "    else:\n",
    "        verdict = \"ðŸ”µ UNDERFITTING\"\n",
    "        status = \"Zu einfach\"\n",
    "    \n",
    "    median_price = df_small['median_house_value'].median()\n",
    "    relative_error = (test_rmse / median_price) * 100\n",
    "    \n",
    "    print(f\"BEWERTUNG: {verdict} ({status})\")\n",
    "    print(f\"Relativer Test-Fehler: {relative_error:.1f}% vom Median-Preis\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "for _, row in results_reg_df.iterrows():\n",
    "    analyze_overfitting_v2(row['Train RMSE'], row['Test RMSE'], row['Model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualisierung: RMSE & Overfitting Gap Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# RMSE Vergleich\n",
    "x = np.arange(len(results_reg_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, results_reg_df['Train RMSE'], width, \n",
    "                label='Train RMSE', alpha=0.8, color='steelblue')\n",
    "bars2 = ax1.bar(x + width/2, results_reg_df['Test RMSE'], width, \n",
    "                label='Test RMSE', alpha=0.8, color='orange')\n",
    "\n",
    "ax1.set_ylabel('RMSE ($)', fontsize=12)\n",
    "ax1.set_title('RMSE Vergleich: Regularisierte Modelle\\n(Dropout + L2 + Early Stopping)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(results_reg_df['Model'], rotation=15)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'${height:,.0f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Overfitting Gap\n",
    "colors = ['green' if gap < 15 else 'orange' if gap < 30 else 'red' \n",
    "          for gap in results_reg_df['Gap %']]\n",
    "\n",
    "bars = ax2.bar(results_reg_df['Model'], results_reg_df['Gap %'], \n",
    "               alpha=0.8, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Overfitting Gap (%)', fontsize=12)\n",
    "ax2.set_title('Overfitting Gap - Regularisierte Modelle\\n(Niedriger = Besser)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.axhline(y=15, color='orange', linestyle='--', alpha=0.5, label='Moderate Grenze (15%)')\n",
    "ax2.axhline(y=30, color='red', linestyle='--', alpha=0.5, label='Kritische Grenze (30%)')\n",
    "ax2.set_xticklabels(results_reg_df['Model'], rotation=15)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.annotate(f'{height:.1f}%',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. FAZIT: Regularisierung vs. Ohne Regularisierung\n",
    "\n",
    "Zusammenfassung der Verbesserungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FAZIT: REGULARISIERUNG WIRKT!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nMit nur 80 Trainingssamples zeigen die regularisierten Modelle:\")\n",
    "print()\n",
    "\n",
    "for _, row in results_reg_df.iterrows():\n",
    "    if row['Gap %'] > 30:\n",
    "        status = \"ðŸ”´ Starkes Overfitting\"\n",
    "    elif row['Gap %'] > 15:\n",
    "        status = \"ðŸŸ¡ Moderates Overfitting\"\n",
    "    else:\n",
    "        status = \"ðŸŸ¢ Guter Fit\"\n",
    "    \n",
    "    print(f\"{row['Model']:15s} -> {status:30s} Gap: {row['Gap %']:6.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"ANGEWANDTE TECHNIKEN:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ“ Dropout Regularization (20-40% je nach ModellgrÃ¶ÃŸe)\")\n",
    "print(\"âœ“ L2 Weight Decay (0.01)\")\n",
    "print(\"âœ“ Early Stopping (Patience: 20 Epochen)\")\n",
    "print(\"âœ“ Learning Rate Scheduling (ReduceLROnPlateau)\")\n",
    "print()\n",
    "print(\"RESULTAT:\")\n",
    "print(f\"â€¢ Durchschnittlicher Overfitting Gap: {results_reg_df['Gap %'].mean():.1f}%\")\n",
    "print(f\"â€¢ Bestes Modell: {results_reg_df.loc[results_reg_df['Gap %'].idxmin(), 'Model']}\")\n",
    "print(f\"  mit Gap von {results_reg_df['Gap %'].min():.1f}%\")\n",
    "print()\n",
    "print(\"Durch Regularisierung wurde das Overfitting deutlich reduziert!\")\n",
    "print(\"Die Modelle generalisieren jetzt besser auf neue Daten.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Predictions vs. Actual - Regularisierte Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "for idx, (name, model) in enumerate(models_reg.items()):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_train_pred_scaled = model(X_train_tensor).squeeze().cpu().numpy()\n",
    "        y_test_pred_scaled = model(X_test_tensor).squeeze().cpu().numpy()\n",
    "    \n",
    "    y_train_pred = y_scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test_pred = y_scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Training Set\n",
    "    ax_train = axes[0, idx]\n",
    "    ax_train.scatter(y_train, y_train_pred, alpha=0.6, s=100, edgecolors='black', color='steelblue')\n",
    "    ax_train.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "                  'r--', linewidth=2, label='Perfect Fit')\n",
    "    ax_train.set_xlabel('Actual Price', fontsize=12)\n",
    "    ax_train.set_ylabel('Predicted Price', fontsize=12)\n",
    "    ax_train.set_title(f'{name}\\nTRAINING SET (n={len(y_train)})', fontsize=12, fontweight='bold')\n",
    "    ax_train.legend()\n",
    "    ax_train.grid(True, alpha=0.3)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    ax_train.text(0.05, 0.95, f'RÂ² = {train_r2:.3f}',\n",
    "                  transform=ax_train.transAxes, verticalalignment='top',\n",
    "                  bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "                  fontsize=11)\n",
    "    \n",
    "    # Test Set\n",
    "    ax_test = axes[1, idx]\n",
    "    ax_test.scatter(y_test, y_test_pred, alpha=0.6, s=100, \n",
    "                    edgecolors='black', color='orange')\n",
    "    ax_test.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                 'r--', linewidth=2, label='Perfect Fit')\n",
    "    ax_test.set_xlabel('Actual Price', fontsize=12)\n",
    "    ax_test.set_ylabel('Predicted Price', fontsize=12)\n",
    "    ax_test.set_title(f'{name}\\nTEST SET (n={len(y_test)})', fontsize=12, fontweight='bold')\n",
    "    ax_test.legend()\n",
    "    ax_test.grid(True, alpha=0.3)\n",
    "    \n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    ax_test.text(0.05, 0.95, f'RÂ² = {test_r2:.3f}',\n",
    "                 transform=ax_test.transAxes, verticalalignment='top',\n",
    "                 bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),\n",
    "                 fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Die Scatter Plots zeigen, wie gut die regularisierten Modelle die Preise vorhersagen.\")\n",
    "print(\"Punkte nahe der roten Linie = gute Predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Learning Rate Verlauf\n",
    "\n",
    "Schauen wir uns an, wie die Learning Rate angepasst wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "histories_lr = [\n",
    "    (history_small_reg, 'Small Model', 'blue'),\n",
    "    (history_medium_reg, 'Medium Model', 'green'),\n",
    "    (history_large_reg, 'Large Model', 'red')\n",
    "]\n",
    "\n",
    "for idx, (history, title, color) in enumerate(histories_lr):\n",
    "    ax = axes[idx]\n",
    "    epochs_range = range(1, len(history['lr']) + 1)\n",
    "    \n",
    "    ax.plot(epochs_range, history['lr'], color=color, linewidth=2)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax.set_title(f'{title}\\nLearning Rate Schedule', fontsize=12, fontweight='bold')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Die Learning Rate wurde automatisch reduziert, wenn kein Fortschritt erzielt wurde.\")\n",
    "print(\"Dies hilft dem Modell, feinere Anpassungen vorzunehmen.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
